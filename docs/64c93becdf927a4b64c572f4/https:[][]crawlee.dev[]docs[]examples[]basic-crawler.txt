Basic crawler | Crawlee
Skip to main content

Crawlee
Crawlee
Docs
Examples
API
Changelog
3.5
Next
Next
3.5
3.5
3.4
3.4
3.3
3.3
3.2
3.2
3.1
3.1
3.0
3.0
2.2
2.2
1.3
1.3


GitHub
Discord
Search⌘K
Search
Search
⌘K

Crawlee
Crawlee
Quick Start
Quick Start
IntroductionSetting upFirst crawlerAdding more URLsReal-world projectCrawlingScrapingSaving dataRefactoring
Introduction

Setting up
Setting up
First crawler
First crawler
Adding more URLs
Adding more URLs
Real-world project
Real-world project
Crawling
Crawling
Scraping
Scraping
Saving data
Saving data
Refactoring
Refactoring
Guides
Guides

ExamplesAccept user inputAdd data to datasetBasic crawlerCheerio crawlerCrawl all links on a websiteCrawl multiple URLsCrawl a website with relative linksCrawl a single URLCrawl a sitemapCrawl some links on a websiteUsing puppeteer-extra and playwright-extraExport entire dataset to one fileFormsHTTP crawlerJSDOM crawlerDataset Map and Reduce methodsPlaywright crawlerUsing Firefox browser with Playwright crawlerCapture a screenshot using PuppeteerPuppeteer crawlerPuppeteer recursive crawlSkipping navigations for certain requests
Examples

Accept user input
Accept user input
Add data to dataset
Add data to dataset
Basic crawler
Basic crawler
Cheerio crawler
Cheerio crawler
Crawl all links on a website
Crawl all links on a website
Crawl multiple URLs
Crawl multiple URLs
Crawl a website with relative links
Crawl a website with relative links
Crawl a single URL
Crawl a single URL
Crawl a sitemap
Crawl a sitemap
Crawl some links on a website
Crawl some links on a website
Using puppeteer-extra and playwright-extra
Using puppeteer-extra and playwright-extra
Export entire dataset to one file
Export entire dataset to one file
Forms
Forms
HTTP crawler
HTTP crawler
JSDOM crawler
JSDOM crawler
Dataset Map and Reduce methods
Dataset Map and Reduce methods
Playwright crawler
Playwright crawler
Using Firefox browser with Playwright crawler
Using Firefox browser with Playwright crawler
Capture a screenshot using Puppeteer
Capture a screenshot using Puppeteer
Puppeteer crawler
Puppeteer crawler
Puppeteer recursive crawl
Puppeteer recursive crawl
Skipping navigations for certain requests
Skipping navigations for certain requests
Upgrading
Upgrading




Examples
Examples
Examples
Basic crawler
Basic crawler
Version: 3.5
Basic crawler
This is the most bare-bones example of using Crawlee, which demonstrates some of its building blocks such as the BasicCrawler. You probably don't need to go this deep though, and it would be better to start with one of the full-featured crawlers
like CheerioCrawler or PlaywrightCrawler.
BasicCrawler
CheerioCrawler
PlaywrightCrawler
The script simply downloads several web pages with plain HTTP requests using the sendRequest utility function (which uses the got-scraping
npm module internally) and stores their raw HTML and URL in the default dataset. In local configuration, the data will be stored as JSON files in
./storage/datasets/default.
sendRequest
got-scraping
import { BasicCrawler, Dataset } from 'crawlee';
import
 
{
 
BasicCrawler
,
 
Dataset
 
}
 
from
 
'crawlee'
;



// Create a BasicCrawler - the simplest crawler that enables

// Create a BasicCrawler - the simplest crawler that enables

// users to implement the crawling logic themselves.

// users to implement the crawling logic themselves.

const crawler = new BasicCrawler({

const
 crawler 
=
 
new
 
BasicCrawler
(
{

    // This function will be called for each URL to crawl.
    
// This function will be called for each URL to crawl.

    async requestHandler({ request, sendRequest, log }) {
    
async
 
requestHandler
(
{
 request
,
 sendRequest
,
 log 
}
)
 
{

        const { url } = request;
        
const
 
{
 url 
}
 
=
 request
;

        log.info(`Processing ${url}...`);
        log
.
info
(
`
Processing 
${
url
}
...
`
)
;



        // Fetch the page HTML via the crawlee sendRequest utility method
        
// Fetch the page HTML via the crawlee sendRequest utility method

        // By default, the method will use the current request that is being handled, so you don't have to
        
// By default, the method will use the current request that is being handled, so you don't have to

        // provide it yourself. You can also provide a custom request if you want.
        
// provide it yourself. You can also provide a custom request if you want.

        const { body } = await sendRequest();
        
const
 
{
 body 
}
 
=
 
await
 
sendRequest
(
)
;



        // Store the HTML and URL to the default dataset.
        
// Store the HTML and URL to the default dataset.

        await Dataset.pushData({
        
await
 
Dataset
.
pushData
(
{

            url,
            url
,

            html: body,
            
html
:
 body
,

        });
        
}
)
;

    },
    
}
,

});

}
)
;



// The initial list of URLs to crawl. Here we use just a few hard-coded URLs.

// The initial list of URLs to crawl. Here we use just a few hard-coded URLs.

await crawler.addRequests([

await
 crawler
.
addRequests
(
[

    'https://www.google.com',
    
'https://www.google.com'
,

    'https://www.example.com',
    
'https://www.example.com'
,

    'https://www.bing.com',
    
'https://www.bing.com'
,

    'https://www.wikipedia.com',
    
'https://www.wikipedia.com'
,

]);

]
)
;



// Run the crawler and wait for it to finish.

// Run the crawler and wait for it to finish.

await crawler.run();

await
 crawler
.
run
(
)
;



console.log('Crawler finished.');

console
.
log
(
'Crawler finished.'
)
;



Last updated on Jul 31, 2023 by renovate[bot]
Jul 31, 2023
renovate[bot]
PreviousAdd data to dataset
NextCheerio crawler
Guides
Guides
Examples
Examples
API reference
API reference
Upgrading to v3
Upgrading to v3
Discord
Discord
Stack Overflow
Stack Overflow
Twitter
Twitter
Apify Platform
Apify Platform
Docusaurus
Docusaurus
GitHub
GitHub
Crawlee is free and open source
Built by
Built by
