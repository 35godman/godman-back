Examples | Crawlee
Skip to main content

Crawlee
Crawlee
Docs
Examples
API
Changelog
3.0
Next
Next
3.5
3.5
3.4
3.4
3.3
3.3
3.2
3.2
3.1
3.1
3.0
3.0
2.2
2.2
1.3
1.3


GitHub
Discord
Search⌘K
Search
Search
⌘K

Crawlee
Crawlee
Quick Start
Quick Start
IntroductionSetting upFirst crawlerAdding more URLsReal-world projectCrawlingScrapingSaving dataRefactoring
Introduction

Setting up
Setting up
First crawler
First crawler
Adding more URLs
Adding more URLs
Real-world project
Real-world project
Crawling
Crawling
Scraping
Scraping
Saving data
Saving data
Refactoring
Refactoring
Guides
Guides

ExamplesAccept user inputAdd data to datasetBasic crawlerCheerio crawlerCrawl all links on a websiteCrawl multiple URLsCrawl a website with relative linksCrawl a single URLCrawl a sitemapCrawl some links on a websiteExport entire dataset to one fileFormsHTTP crawlerJSDOM crawlerDataset Map and Reduce methodsPlaywright crawlerUsing Firefox browser with Playwright crawlerCapture a screenshot using PuppeteerPuppeteer crawlerPuppeteer recursive crawlSkipping navigations for certain requests
Examples

Accept user input
Accept user input
Add data to dataset
Add data to dataset
Basic crawler
Basic crawler
Cheerio crawler
Cheerio crawler
Crawl all links on a website
Crawl all links on a website
Crawl multiple URLs
Crawl multiple URLs
Crawl a website with relative links
Crawl a website with relative links
Crawl a single URL
Crawl a single URL
Crawl a sitemap
Crawl a sitemap
Crawl some links on a website
Crawl some links on a website
Export entire dataset to one file
Export entire dataset to one file
Forms
Forms
HTTP crawler
HTTP crawler
JSDOM crawler
JSDOM crawler
Dataset Map and Reduce methods
Dataset Map and Reduce methods
Playwright crawler
Playwright crawler
Using Firefox browser with Playwright crawler
Using Firefox browser with Playwright crawler
Capture a screenshot using Puppeteer
Capture a screenshot using Puppeteer
Puppeteer crawler
Puppeteer crawler
Puppeteer recursive crawl
Puppeteer recursive crawl
Skipping navigations for certain requests
Skipping navigations for certain requests
Upgrading
Upgrading


3.0
latest version
latest version


Examples
Examples
Version: 3.0
Examples
📄️ Accept user inputThis example accepts and logs user input:
📄️ Accept user input
This example accepts and logs user input:
📄️ Add data to datasetThis example saves data to the default dataset. If the dataset doesn't exist, it will be created.
📄️ Add data to dataset
This example saves data to the default dataset. If the dataset doesn't exist, it will be created.
📄️ Basic crawlerThis is the most bare-bones example of using Crawlee, which demonstrates some of its building blocks such as the BasicCrawler. You probably don't need to go this deep though, and it would be better to start with one of the full-featured crawlers
📄️ Basic crawler
This is the most bare-bones example of using Crawlee, which demonstrates some of its building blocks such as the BasicCrawler. You probably don't need to go this deep though, and it would be better to start with one of the full-featured crawlers
📄️ Cheerio crawlerThis example demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the Cheerio library and extract some data from it: the page title and all h1 tags.
📄️ Cheerio crawler
This example demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the Cheerio library and extract some data from it: the page title and all h1 tags.
📄️ Crawl all links on a websiteThis example uses the enqueueLinks() method to add new links to the RequestQueue
📄️ Crawl all links on a website
This example uses the enqueueLinks() method to add new links to the RequestQueue
📄️ Crawl multiple URLsThis example crawls the specified list of URLs.
📄️ Crawl multiple URLs
This example crawls the specified list of URLs.
📄️ Crawl a website with relative linksWhen crawling a website, you may encounter different types of links present that you may want to crawl.
📄️ Crawl a website with relative links
When crawling a website, you may encounter different types of links present that you may want to crawl.
📄️ Crawl a single URLThis example uses the got-scraping npm package
📄️ Crawl a single URL
This example uses the got-scraping npm package
📄️ Crawl a sitemapThis example downloads and crawls the URLs from a sitemap, by using the downloadListOfUrls utility method provided by the @crawlee/utils module.
📄️ Crawl a sitemap
This example downloads and crawls the URLs from a sitemap, by using the downloadListOfUrls utility method provided by the @crawlee/utils module.
📄️ Crawl some links on a websiteThis CheerioCrawler example uses the globs property in the enqueueLinks() method to only add links to the RequestQueue queue if they match the specified pattern.
📄️ Crawl some links on a website
This CheerioCrawler example uses the globs property in the enqueueLinks() method to only add links to the RequestQueue queue if they match the specified pattern.
📄️ Export entire dataset to one fileThis Dataset example uses the exportToValue function to export the entire default dataset to a single CSV file into a key-value store named "my-data".
📄️ Export entire dataset to one file
This Dataset example uses the exportToValue function to export the entire default dataset to a single CSV file into a key-value store named "my-data".
📄️ FormsThis example demonstrates how to use PuppeteerCrawler to
📄️ Forms
This example demonstrates how to use PuppeteerCrawler to
📄️ HTTP crawlerThis example demonstrates how to use HttpCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, and save HTML.
📄️ HTTP crawler
This example demonstrates how to use HttpCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, and save HTML.
📄️ JSDOM crawlerThis example demonstrates how to use JSDOMCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the jsdom DOM implementation and extract some data from it: the page title and all h1 tags.
📄️ JSDOM crawler
This example demonstrates how to use JSDOMCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the jsdom DOM implementation and extract some data from it: the page title and all h1 tags.
📄️ Dataset Map and Reduce methodsThis example shows an easy use-case of the Dataset map
📄️ Dataset Map and Reduce methods
This example shows an easy use-case of the Dataset map
📄️ Playwright crawlerThis example demonstrates how to use PlaywrightCrawler in combination with RequestQueue to recursively scrape the Hacker News website using headless Chrome / Playwright.
📄️ Playwright crawler
This example demonstrates how to use PlaywrightCrawler in combination with RequestQueue to recursively scrape the Hacker News website using headless Chrome / Playwright.
📄️ Using Firefox browser with Playwright crawlerThis example demonstrates how to use PlaywrightCrawler with headless Firefox browser.
📄️ Using Firefox browser with Playwright crawler
This example demonstrates how to use PlaywrightCrawler with headless Firefox browser.
📄️ Capture a screenshot using PuppeteerUsing Puppeteer directly
📄️ Capture a screenshot using Puppeteer
Using Puppeteer directly
📄️ Puppeteer crawlerThis example demonstrates how to use PuppeteerCrawler in combination
📄️ Puppeteer crawler
This example demonstrates how to use PuppeteerCrawler in combination
📄️ Puppeteer recursive crawlRun the following example to perform a recursive crawl of a website using PuppeteerCrawler.
📄️ Puppeteer recursive crawl
Run the following example to perform a recursive crawl of a website using PuppeteerCrawler.
📄️ Skipping navigations for certain requestsWhile crawling a website, you may encounter certain resources you'd like to save, but don't need the full power of a crawler to do so (like images delivered through a CDN).
📄️ Skipping navigations for certain requests
While crawling a website, you may encounter certain resources you'd like to save, but don't need the full power of a crawler to do so (like images delivered through a CDN).
PreviousApify Platform
NextAccept user input
Guides
Guides
Examples
Examples
API reference
API reference
Upgrading to v3
Upgrading to v3
Discord
Discord
Stack Overflow
Stack Overflow
Twitter
Twitter
Apify Platform
Apify Platform
Docusaurus
Docusaurus
GitHub
GitHub
Crawlee is free and open source
Built by
Built by
