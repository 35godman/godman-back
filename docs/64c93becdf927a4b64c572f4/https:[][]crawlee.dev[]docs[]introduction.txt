Introduction | Crawlee
Skip to main content

Crawlee
Crawlee
Docs
Examples
API
Changelog
3.5
Next
Next
3.5
3.5
3.4
3.4
3.3
3.3
3.2
3.2
3.1
3.1
3.0
3.0
2.2
2.2
1.3
1.3


GitHub
Discord
SearchâŒ˜K
Search
Search
âŒ˜K

Crawlee
Crawlee
Quick Start
Quick Start
IntroductionSetting upFirst crawlerAdding more URLsReal-world projectCrawlingScrapingSaving dataRefactoring
Introduction

Setting up
Setting up
First crawler
First crawler
Adding more URLs
Adding more URLs
Real-world project
Real-world project
Crawling
Crawling
Scraping
Scraping
Saving data
Saving data
Refactoring
Refactoring
Guides
Guides

Examples
Examples

Upgrading
Upgrading




Introduction
Introduction
Version: 3.5
On this page
Introduction
Crawlee covers your crawling and scraping end-to-end and helps you build reliable scrapers. Fast.
build reliable scrapers. Fast.
Your crawlers will appear human-like and fly under the radar of modern bot protections even with the default configuration. Crawlee gives you the tools to crawl the web for links, scrape data and persistently store it in machine-readable formats, without having to worry about the technical details. And thanks to rich configuration options, you can tweak almost any aspect of Crawlee to suit your project's needs if the default settings don't cut it.
What you will learnâ€‹
â€‹
The goal of the introduction is to provide a step-by-step guide to the most important features of Crawlee. It will walk you through creating the simplest of crawlers that only prints text to console, all the way up to a full-featured scraper that collects links from a website and extracts data.
ðŸ›  Featuresâ€‹
â€‹
Single interface for HTTP and headless browser crawling
HTTP and headless browser
Persistent queue for URLs to crawl (breadth & depth first)
queue
Pluggable storage of both tabular data and files
storage
Automatic scaling with available system resources
scaling
Integrated proxy rotation and session management
proxy rotation
Lifecycles customizable with hooks
hooks
CLI to bootstrap your projects
CLI
Configurable routing, error handling and retries
routing
error handling
retries
Dockerfiles ready to deploy
Dockerfiles
Written in TypeScript with generics
TypeScript
ðŸ‘¾ HTTP crawlingâ€‹
â€‹
Zero config HTTP2 support, even for proxies
HTTP2 support
Automatic generation of browser-like headers
browser-like headers
Replication of browser TLS fingerprints
TLS fingerprints
Integrated fast HTML parsers. Cheerio and JSDOM
HTML parsers
Yes, you can scrape JSON APIs as well
JSON APIs
ðŸ’» Real browser crawlingâ€‹
â€‹
JavaScript rendering and screenshots
rendering
screenshots
Headless and headful support
Headless
headful
Zero-config generation of human-like fingerprints
human-like fingerprints
Automatic browser management
browser management
Use Playwright and Puppeteer with the same interface
Playwright
Puppeteer
Chrome, Firefox, Webkit and many others
Chrome
Firefox
Webkit
Next lessonâ€‹
â€‹
In the next lesson you will install Crawlee and learn how to bootstrap projects with the Crawlee CLI.
Last updated on Jul 31, 2023 by renovate[bot]
Jul 31, 2023
renovate[bot]
PreviousQuick Start
NextSetting up
What you will learn
What you will learn
ðŸ›  FeaturesðŸ‘¾ HTTP crawlingðŸ’» Real browser crawling
ðŸ›  Features
ðŸ‘¾ HTTP crawling
ðŸ‘¾ HTTP crawling
ðŸ’» Real browser crawling
ðŸ’» Real browser crawling
Next lesson
Next lesson
Guides
Guides
Examples
Examples
API reference
API reference
Upgrading to v3
Upgrading to v3
Discord
Discord
Stack Overflow
Stack Overflow
Twitter
Twitter
Apify Platform
Apify Platform
Docusaurus
Docusaurus
GitHub
GitHub
Crawlee is free and open source
Built by
Built by
