Cheerio crawler | Crawlee
Skip to main content

Crawlee
Crawlee
Docs
Examples
API
Changelog
3.5
Next
Next
3.5
3.5
3.4
3.4
3.3
3.3
3.2
3.2
3.1
3.1
3.0
3.0
2.2
2.2
1.3
1.3


GitHub
Discord
Search⌘K
Search
Search
⌘K

Crawlee
Crawlee
Quick Start
Quick Start
IntroductionSetting upFirst crawlerAdding more URLsReal-world projectCrawlingScrapingSaving dataRefactoring
Introduction

Setting up
Setting up
First crawler
First crawler
Adding more URLs
Adding more URLs
Real-world project
Real-world project
Crawling
Crawling
Scraping
Scraping
Saving data
Saving data
Refactoring
Refactoring
Guides
Guides

ExamplesAccept user inputAdd data to datasetBasic crawlerCheerio crawlerCrawl all links on a websiteCrawl multiple URLsCrawl a website with relative linksCrawl a single URLCrawl a sitemapCrawl some links on a websiteUsing puppeteer-extra and playwright-extraExport entire dataset to one fileFormsHTTP crawlerJSDOM crawlerDataset Map and Reduce methodsPlaywright crawlerUsing Firefox browser with Playwright crawlerCapture a screenshot using PuppeteerPuppeteer crawlerPuppeteer recursive crawlSkipping navigations for certain requests
Examples

Accept user input
Accept user input
Add data to dataset
Add data to dataset
Basic crawler
Basic crawler
Cheerio crawler
Cheerio crawler
Crawl all links on a website
Crawl all links on a website
Crawl multiple URLs
Crawl multiple URLs
Crawl a website with relative links
Crawl a website with relative links
Crawl a single URL
Crawl a single URL
Crawl a sitemap
Crawl a sitemap
Crawl some links on a website
Crawl some links on a website
Using puppeteer-extra and playwright-extra
Using puppeteer-extra and playwright-extra
Export entire dataset to one file
Export entire dataset to one file
Forms
Forms
HTTP crawler
HTTP crawler
JSDOM crawler
JSDOM crawler
Dataset Map and Reduce methods
Dataset Map and Reduce methods
Playwright crawler
Playwright crawler
Using Firefox browser with Playwright crawler
Using Firefox browser with Playwright crawler
Capture a screenshot using Puppeteer
Capture a screenshot using Puppeteer
Puppeteer crawler
Puppeteer crawler
Puppeteer recursive crawl
Puppeteer recursive crawl
Skipping navigations for certain requests
Skipping navigations for certain requests
Upgrading
Upgrading




Examples
Examples
Examples
Cheerio crawler
Cheerio crawler
Version: 3.5
Cheerio crawler
This example demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the Cheerio library and extract some data from it: the page title and all h1 tags.
CheerioCrawler
Cheerio library
import { Dataset, CheerioCrawler, log, LogLevel } from 'crawlee';
import
 
{
 
Dataset
,
 
CheerioCrawler
,
 log
,
 
LogLevel
 
}
 
from
 
'crawlee'
;



// Crawlers come with various utilities, e.g. for logging.

// Crawlers come with various utilities, e.g. for logging.

// Here we use debug level of logging to improve the debugging experience.

// Here we use debug level of logging to improve the debugging experience.

// This functionality is optional!

// This functionality is optional!

log.setLevel(LogLevel.DEBUG);
log
.
setLevel
(
LogLevel
.
DEBUG
)
;



// Create an instance of the CheerioCrawler class - a crawler

// Create an instance of the CheerioCrawler class - a crawler

// that automatically loads the URLs and parses their HTML using the cheerio library.

// that automatically loads the URLs and parses their HTML using the cheerio library.

const crawler = new CheerioCrawler({

const
 crawler 
=
 
new
 
CheerioCrawler
(
{

    // The crawler downloads and processes the web pages in parallel, with a concurrency
    
// The crawler downloads and processes the web pages in parallel, with a concurrency

    // automatically managed based on the available system memory and CPU (see AutoscaledPool class).
    
// automatically managed based on the available system memory and CPU (see AutoscaledPool class).

    // Here we define some hard limits for the concurrency.
    
// Here we define some hard limits for the concurrency.

    minConcurrency: 10,
    
minConcurrency
:
 
10
,

    maxConcurrency: 50,
    
maxConcurrency
:
 
50
,



    // On error, retry each page at most once.
    
// On error, retry each page at most once.

    maxRequestRetries: 1,
    
maxRequestRetries
:
 
1
,



    // Increase the timeout for processing of each page.
    
// Increase the timeout for processing of each page.

    requestHandlerTimeoutSecs: 30,
    
requestHandlerTimeoutSecs
:
 
30
,



    // Limit to 10 requests per one crawl
    
// Limit to 10 requests per one crawl

    maxRequestsPerCrawl: 10,
    
maxRequestsPerCrawl
:
 
10
,



    // This function will be called for each URL to crawl.
    
// This function will be called for each URL to crawl.

    // It accepts a single parameter, which is an object with options as:
    
// It accepts a single parameter, which is an object with options as:

    // https://crawlee.dev/api/cheerio-crawler/interface/CheerioCrawlerOptions#requestHandler
    
// https://crawlee.dev/api/cheerio-crawler/interface/CheerioCrawlerOptions#requestHandler

    // We use for demonstration only 2 of them:
    
// We use for demonstration only 2 of them:

    // - request: an instance of the Request class with information such as the URL that is being crawled and HTTP method
    
// - request: an instance of the Request class with information such as the URL that is being crawled and HTTP method

    // - $: the cheerio object containing parsed HTML
    
// - $: the cheerio object containing parsed HTML

    async requestHandler({ request, $ }) {
    
async
 
requestHandler
(
{
 request
,
 $ 
}
)
 
{

        log.debug(`Processing ${request.url}...`);
        log
.
debug
(
`
Processing 
${
request
.
url
}
...
`
)
;



        // Extract data from the page using cheerio.
        
// Extract data from the page using cheerio.

        const title = $('title').text();
        
const
 title 
=
 
$
(
'title'
)
.
text
(
)
;

        const h1texts: { text: string }[] = [];
        
const
 
h1texts
:
 
{
 
text
:
 string 
}
[
]
 
=
 
[
]
;

        $('h1').each((index, el) => {
        
$
(
'h1'
)
.
each
(
(
index
,
 el
)
 
=>
 
{

            h1texts.push({
            h1texts
.
push
(
{

                text: $(el).text(),
                
text
:
 
$
(
el
)
.
text
(
)
,

            });
            
}
)
;

        });
        
}
)
;



        // Store the results to the dataset. In local configuration,
        
// Store the results to the dataset. In local configuration,

        // the data will be stored as JSON files in ./storage/datasets/default
        
// the data will be stored as JSON files in ./storage/datasets/default

        await Dataset.pushData({
        
await
 
Dataset
.
pushData
(
{

            url: request.url,
            
url
:
 request
.
url
,

            title,
            title
,

            h1texts,
            h1texts
,

        });
        
}
)
;

    },
    
}
,



    // This function is called if the page processing failed more than maxRequestRetries + 1 times.
    
// This function is called if the page processing failed more than maxRequestRetries + 1 times.

    failedRequestHandler({ request }) {
    
failedRequestHandler
(
{
 request 
}
)
 
{

        log.debug(`Request ${request.url} failed twice.`);
        log
.
debug
(
`
Request 
${
request
.
url
}
 failed twice.
`
)
;

    },
    
}
,

});

}
)
;



// Run the crawler and wait for it to finish.

// Run the crawler and wait for it to finish.

await crawler.run([

await
 crawler
.
run
(
[

    'https://crawlee.dev',
    
'https://crawlee.dev'
,

]);

]
)
;



log.debug('Crawler finished.');
log
.
debug
(
'Crawler finished.'
)
;



Last updated on Jul 31, 2023 by renovate[bot]
Jul 31, 2023
renovate[bot]
PreviousBasic crawler
NextCrawl all links on a website
Guides
Guides
Examples
Examples
API reference
API reference
Upgrading to v3
Upgrading to v3
Discord
Discord
Stack Overflow
Stack Overflow
Twitter
Twitter
Apify Platform
Apify Platform
Docusaurus
Docusaurus
GitHub
GitHub
Crawlee is free and open source
Built by
Built by
