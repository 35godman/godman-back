Opens in a new window Opens an external website Opens an external website in a
new window
<!---->Close this dialog<!---->
This website utilizes technologies such as cookies to enable essential site
functionality, as well as for analytics, personalization, and targeted
advertising purposes. To learn more, view the following link:



<!---->Close Cookie Preferences<!---->


Product
Solutions
Resources
Company



REGULARIZATION IN NEURAL NETWORKS

Jun 30, 2023

Author
[/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&w=3840&q=100]

Bala Priya C

Technical Writer

--------------------------------------------------------------------------------

Jump to section

--------------------------------------------------------------------------------

Regularization in neural networks
[https://cdn.sanity.io/images/vr8gru94/production/e455528b7144d1fa75b14be1f2b6c3cb5fa1e47e-1200x627.png]


Neural networks can learn to represent complex relationships between network
inputs and outputs. This representational power helps them perform better than
traditional machine learning algorithms in computer vision and natural language
processing tasks. However, one of the challenges associated with training neural
networks is overfitting.

When a neural network overfits on the training dataset, it learns an overly
complex representation that models the training dataset too well. As a result,
it performs exceptionally well on the training dataset but generalizes poorly to
unseen test data.

Regularization techniques help improve a neural network’s generalization ability
by reducing overfitting. They do this by minimizing needless complexity and
exposing the network to more diverse data. This article will cover common
regularization techniques:

 * Early stopping
 * L1 and L2 regularization
 * Data augmentation
 * Addition of noise
 * Dropout

Let’s get started!


EARLY STOPPING

Early stopping is one of the simplest and most intuitive regularization
techniques. It involves stopping the training of the neural network at an
earlier epoch; hence the name early stopping.

But how and when do we stop? As you train the neural network over many epochs,
the training error decreases.

If the training error becomes too low and reaches arbitrarily close to zero,
then the network is sure to overfit on the training dataset. Such a neural
network is a that performs badly on test data that it has never seen before
despite its near-perfect performance on the training samples.

Therefore, heuristically, if we can prevent the training loss from becoming
arbitrarily low, the model is less likely to overfit on the training dataset,
and will generalize better.

So how do we do it in practice? You can monitor one of the following:

 * The change in metrics such as validation error and validation accuracy
 * The change in the weight vector


MONITORING THE CHANGE IN VALIDATION ERROR

A simple approach is to monitor metrics such as validation error and validation
accuracy as the neural network training proceeds, and use them to decide when to
stop.

If we find that the validation error is not decreasing significantly or is
increasing over a window of epochs, say p epochs, we can stop training. We can
as well lower the learning rate and train for a few more epochs before stopping.

Error Change and Early Stopping
[https://cdn.sanity.io/images/vr8gru94/production/dfa532f08c7b0720c72306d327a82f78f7e17f5d-1000x843.png]
Error Change and Early Stopping

Equivalently, you can think in terms of the neural network’s accuracy on the
training and validation datasets. Stopping early when the validation error
starts increasing (or is no longer decreasing) is equivalent to stopping when
the validation accuracy starts decreasing.

Monitoring the Validation Accuracy for Early Stopping
[https://cdn.sanity.io/images/vr8gru94/production/284c41fe112a6f13042792fe0727b13802ae7380-1000x810.png]
Monitoring the Validation Accuracy for Early Stopping


MONITORING THE CHANGE IN THE WEIGHT VECTOR

Another way to know when to stop is to monitor the change in weights of the
network. Let wt\textbf{w}_twt and wt−k\textbf{w} _{t-k}wt−k denote the weight
vectors at epochs ttt and t−kt-kt−k, respectively.

We can compute the L2 norm of the difference vector wt−wt−k\textbf{w}_t -
\textbf{w} _{t-k}wt −wt−k . We can stop training if this quantity is
sufficiently small, say, less than ϵ\epsilonϵ.

∣∣wt−wt−k∣∣2<ϵ||\textbf{w}_t - \textbf{w} _{t-k}||_2 < \epsilon∣∣wt −wt−k ∣∣2 <ϵ

--------------------------------------------------------------------------------

Note: We’ll review Lp norms in greater detail in the section on L1 and L2
regularization. For now, you can think of the L2 norm as a non-negative quantity
that captures the distance between any two vectors in a vector space.

--------------------------------------------------------------------------------

But this approach of using the norm of the difference vector is not very
reliable. Why? Certain weights might have changed a lot in the last k epochs,
while some weights may have negligible changes. Therefore, the norm of the
resultant difference vector can be small despite the drastic change in certain
components of the weight vector.

A better approach is to compute the change in individual components of the
weight vector. If the maximum change (across all components) is less than
ϵ\epsilonϵ, we can conclude that the weights are not changing significantly, so
we can stop the training of the neural network.

maxi∣wti−wt−ki∣<ϵmax_{i} |{\textbf{w}_t}^{i} - {\textbf{w} _{t-k}}^{i}| <
\epsilonmaxi ∣wt i−wt−k i∣<ϵ


DATA AUGMENTATION

Data augmentation is a regularization technique that helps a neural network
generalize better by exposing it to a more diverse set of training examples. As
deep neural networks require a large training dataset, data augmentation is also
helpful when we have insufficient data to train a neural network.

Let’s take the example of image data augmentation. Suppose we have a dataset
with N training examples across C classes. We can apply certain transformations
to these N images to construct a larger dataset.

Data Augmentation
[https://cdn.sanity.io/images/vr8gru94/production/a12e8801e2f47ee8913c3931ba4c5d69a4da2628-1000x317.png]
Data Augmentation

What is a valid transformation? Any operation that does not alter the original
label is a valid transformation. For example, a panda is a panda–whether it’s
facing right or left, located near the center of the image or one of the
corners.

In summary: we can apply any label-invariant transformation to perform data
augmentation [1]. The following are some examples:

 * Color space transformations such as change of pixel intensities
 * Rotation and mirroring
 * Noise injection, distortion, and blurring


NEWER APPROACHES TO IMAGE DATA AUGMENTATION

In addition to the basic color space and geometric image transformations, there
are newer image augmentation techniques.

Mixup is a regularization technique that uses a convex combination of existing
inputs to augment the dataset [2].

Suppose xi\textbf{x}_ixi and xj\textbf{x}_jxj are input samples belonging to the
classes i and j, respectively; yi\textbf{y}_iyi and yj\textbf{y}_jyj are the
one-hot vectors corresponding to the class labels i and j, respectively. A new
image is formed by taking a of xi\textbf{x}_ixi and xj\textbf{x}_jxj :

x~=λxi+(1−λ)xj\tilde{x} = \lambda \textbf{x}_i + (1- \lambda) \textbf{x}_jx~=λxi
+(1−λ)xj
y~=λyi+(1−λ)yj\tilde{y} = \lambda \textbf{y}_i + (1- \lambda) \textbf{y}_jy~
=λyi +(1−λ)yj
∀λ∈[0,1]\forall \lambda \in [0,1]∀λ∈[0,1]

A few other approaches to data augmentation include Cutout, CutMix, and AugMix.
Cutout involves the random removal of portions of an input image during training
[3]. CutMix replaces the removed sections with parts of another image [4].

AugMix is a regularization technique that makes a neural network robust to
distribution change [5]. Unlike mixup that uses images from two different
classes, AugMix performs a series of transformations on the same image, and then
uses a composition of these transformed images to get the resultant image.

How AugMix Works
[https://cdn.sanity.io/images/vr8gru94/production/b9e10cf4e0b69c92426fc686470e85008db934ac-1000x450.png]
How AugMix Works


L1 AND L2 REGULARIZATION

In general, Lp norms (for p>=1) penalize larger weights. They force the norm of
the weight vector to stay sufficiently small. The Lp norm of a vector
x\textbf{x}x in n-dimensional space is given by:

Lp(x)=∣∣x∣∣p=(∑i=1n∣xi∣p)1/pLp(\textbf{x}) = ||\textbf{x}||_p = \left(\sum
_{i=1}^{n} |x_i|^{p}\right)^{1/p}Lp(x)=∣∣x∣∣p =(i=1∑n ∣xi ∣p)1/p

L1 norm: When p=1, we get L1 norm, the sum of the absolute values of the
components in the vector:

L1(x)=∣∣x∣∣1=∑i=1n∣xi∣L_1(\textbf{x}) = ||\textbf{x}||_1 = \sum _{i=1}^{n}
|x_i|L1 (x)=∣∣x∣∣1 =i=1∑n ∣xi ∣

L2 norm: When p=2, we get L2 norm, the Euclidean distance of the point from the
origin in n-dimensional vector space:

L2(x)=∣∣x∣∣2=(∑i=1n∣xi∣2)1/2L_2(\textbf{x}) = ||\textbf{x}||_2 = (\sum
_{i=1}^{n} |x_i|^{2})^{1/2}L2 (x)=∣∣x∣∣2 =(i=1∑n ∣xi ∣2)1/2

We’ll focus on L1 and L2 regularization. The unit L1 and L2 norm balls in
two-dimensional plane are shown:

Unit Lp Norm Balls in 2D space for p = 1, 2
[https://cdn.sanity.io/images/vr8gru94/production/a7702205b51fd7bc8da65f1b2db58e1065903cc2-1000x676.png]
Unit Lp Norm Balls in 2D space for p = 1, 2


HOW AND WHY REGULARIZATION WORKS

Let L\mathcal{L}L and L~\tilde{\mathcal{L}}L~ be the loss functions without and
with regularization, respectively. The regularized loss function is given by:

L~(w)=L(w)+α∣∣w∣∣1\tilde{\mathcal{L}}(\textbf{w}) = \mathcal{L}(\textbf{w})
+{\alpha} ||\textbf{w}||_1L~(w)=L(w)+α∣∣w∣∣1

where, α\alphaα is the regularization constant. Suppose α\alphaα is a
sufficiently large constant.

When the weights become too large, the second term α∣∣w∣∣1\alpha||w||_1α∣∣w∣∣1
increases. However, as the goal of optimization is to minimize the loss
function, the weights should decrease.

In particular, the L1 regularization promotes sparsity by forcing weights to
zero. But why? Let’s take the example of 2D space: the L1 norm ball is a
rhombus, and the regularized loss function is minimized at one of the vertices
of the rhombus (where one of the coordinates is zero).

L1 Regularization Promotes Sparsity
[https://cdn.sanity.io/images/vr8gru94/production/76d6078eb1e915fd3cbff13c892932660b8f7ae4-1000x651.png]
L1 Regularization Promotes Sparsity

In n-dimensional space, the loss function is minimized at one of the vertices of
an n-dimensional rhombohedron; all other coordinates are zero.

Similarly, the regularized loss function with L2 regularization (also called L2
weight decay) is given by:

L~(w)=L(w)+α2∣∣w∣∣22\tilde{\mathcal{L}}(\textbf{w}) = \mathcal{L}(\textbf{w}) +
\frac{\alpha}{2} {||\textbf{w}||_2}^2L~(w)=L(w)+2α ∣∣w∣∣2 2

As with L1 regularization, if the weights become too large,
∣∣w∣∣2||\textbf{w}||^2∣∣w∣∣2 increases, and consequently, the second term in the
above equation increases. However, as the objective is to minimize the loss
function, the algorithm favors smaller weights.

Even though the above equation looks like an unconstrained optimization problem,
there is a constraint on the norm of the weight vector in a particular layer.

min⁡wL(w)\min_{\textbf{w}} \mathcal{L}(\textbf{w})wmin L(w)
s.t. ∣∣w∣∣2≤ks.t. \text{ } ||\textbf{w}||_2 \leq ks.t. ∣∣w∣∣2 ≤k

Unlike L1 regularization that forces weights to zero, L2 regularization shrinks
weights while ensuring that important components of the weight vector are larger
than the others. For a detailed mathematical analysis of how the regularized
weight vector is related to the weight vector without regularization, refer to
[9].


ADDITION OF NOISE

Another regularization approach is the addition of noise. You can add noise to
the input, the output labels, or the gradients of the neural network.


ADDING NOISE TO INPUT DATA

When using the sum of squares loss function, adding Gaussian noise to the inputs
is equivalent to L2 regularization [6].

To understand this equivalence, let’s assume a simple network with input layer
and weights:

Adding Noise to Inputs
[https://cdn.sanity.io/images/vr8gru94/production/bd085bc9e929619936dec4408ddec4b0c83c0b63-1000x682.png]
Adding Noise to Inputs

For each input sample xix_ixi , we add noise ϵi\epsilon_iϵi sampled from a
normal distribution with zero mean and variance σ2\sigma^2σ2. The ϵi\epsilon_iϵi
’s are all independent. The output y~\tilde{y}y~ is given as follows:

Let xi~=xi+ϵiLet \text{ }\tilde{x_i} = x_i + \epsilon_iLet xi ~ =xi +ϵi
y~=∑i=1nwixi~\tilde{y} = \sum_{i=1}^{n} w_i \tilde{x_i}y~ =i=1∑n wi xi ~
Substituting xi~Substituting \text{ } \tilde{x_i}Substituting xi ~
y~=∑i=1nwi(xi+ϵi)\tilde{y} = \sum_{i=1}^{n} w_i (x_i + \epsilon_i)y~ =i=1∑n wi
(xi +ϵi )
y~=∑i=1nwixi+∑i=1nwiϵi\tilde{y} = \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} w_i
\epsilon_iy~ =i=1∑n wi xi +i=1∑n wi ϵi

Let y^\hat{y}y^ denote the output in the absence of noise. Substituting the
value of y^\hat{y}y^ , we have:

Let y^=∑i=1nwixiLet \text{ } \hat{y} = \sum_{i=1}^{n} w_i x_iLet y^ =i=1∑n wi xi
  ⟹  y~=y^+∑i=1nwiϵi\implies \tilde{y} = \hat{y} + \sum_{i=1}^{n} w_i
\epsilon_i⟹y~ =y^ +i=1∑n wi ϵi

If yyy is the actual output, the expected sum of squares error is given by:

E[(y~−y)2]=E[(y^+∑i=1nwiϵi−y)2]\mathbb{E}[(\tilde{y} - y)^2] =
\mathbb{E}\left[\left(\hat{y} + \sum_{i=1}^{n} w_i \epsilon_i -
y\right)^2\right]E[(y~ −y)2]=E (y^ +i=1∑n wi ϵi −y)2
=E[((y^−y)+(∑i=1nwiϵi))2]= \mathbb{E}\left[\left((\hat{y} - y) +
\left(\sum_{i=1}^{n} w_i \epsilon_i\right) \right)^2\right]=E ((y^ −y)+(i=1∑n wi
ϵi ))2
=E[(y^−y)2]+E[2(y^−y)∑i=1nwiϵi]+E[(∑i=1nwiϵi)2]= \mathbb{E}[(\hat{y} - y)^2] +
\mathbb{E}\left[2(\hat{y} - y) \sum_{i=1}^{n} w_i \epsilon_i\right] +
\mathbb{E}\left[\left(\sum_{i=1}^{n} w_i \epsilon_i\right)^2\right]=E[(y^
−y)2]+E[2(y^ −y)i=1∑n wi ϵi ]+E (i=1∑n wi ϵi )2

The second term in the above sum goes to zero as E(ϵi)=0\mathbb{E}(\epsilon_i) =
0E(ϵi )=0.

 * Using the independence of ϵi\epsilon_iϵi , terms in the summation that are of
   the form k.E(ϵiϵj)k.\mathbb{E}(\epsilon_i\epsilon_j)k.E(ϵi ϵj ) vanish:
   E(ϵiϵj)=E(ϵi)E(ϵj)=0\mathbb{E}(\epsilon_i\epsilon_j) = \mathbb{E}(\epsilon_i)
   \mathbb{E}(\epsilon_j) = 0E(ϵi ϵj )=E(ϵi )E(ϵj )=0 for all i≠ji \neq ji=j.
 * Further, E(ϵi2)=σ2\mathbb{E}({\epsilon_i}^2) = \sigma^2E(ϵi 2)=σ2 as
   ϵi\epsilon_iϵi is a zero-mean random variable.

E[(y~−y)2]=E[(y^−y)2]+E[(∑i=1nwiϵi)2]\mathbb{E}[(\tilde{y} - y)^2] =
\mathbb{E}[(\hat{y} - y)^2] + \mathbb{E}\left[\left(\sum_{i=1}^{n} w_i
\epsilon_i\right)^2\right]E[(y~ −y)2]=E[(y^ −y)2]+E (i=1∑n wi ϵi )2
=E[(y^−y)2]+E[∑i=1nwi2ϵi2]= \mathbb{E}[(\hat{y} - y)^2] +
\mathbb{E}\left[\sum_{i=1}^{n} w_i^{2} \epsilon_i^{2}\right]=E[(y^ −y)2]+E[i=1∑n
wi2 ϵi2 ]
=E[(y^−y)2]+σ2∑i=1nwi2= \mathbb{E}[(\hat{y} - y)^2] + \sigma^{2}\sum_{i=1}^{n}
w_i^{2}=E[(y^ −y)2]+σ2i=1∑n wi2

From the above equation, we see that the loss has the same form as L2
regularization.


ADDING NOISE TO OUTPUT LABELS

Adding noise to the output labels prevents the network from memorizing the
training dataset by introducing perturbations in the output labels. Here are
some techniques:

DISTURBLABEL

The DisturbLabel algorithm, proposed in [7], works as follows. Suppose there are
N output classes: {1,2,3,…,N}. DisturbLabel works as follows:

 * Each training example is disturbed with the probability α\alphaα.
 * For each disturbed sample, the label is drawn from a uniform distribution
   over {1,2,3,…,N} irrespective of the true label.

LABEL SMOOTHING

Another approach to introducing noise in the output labels is using label
smoothing. This regularization technique accounts for mislabeled samples in the
training dataset.

Suppose there are N total classes and the given label is k. In this case, the
one-hot vector has 1 at the index corresponding to the class k and 0 elsewhere.

Class Label123...k...NWithout Label Smoothing000...1...0

When we perform label smoothing, we replace the hard target 1 with ϵ\epsilonϵ.
To ensure the total probability is 1, we can set the other N-1 indices to
1−ϵN−1\frac{1-\epsilon}{N-1}N−11−ϵ .

With label smoothing
[https://cdn.sanity.io/images/vr8gru94/production/acf1898dd4b32709ccf09563d5a248f04efcd883-1344x286.png]



ADDING NOISE TO GRADIENTS

If the gradient vector at step ttt is gt\textbf{g}_tgt , we update it by adding
noise with zero mean and variance σt2{\sigma _{t}}^2σt 2:

gt=gt+N(0,σt2)\textbf{g}_t = \textbf{g}_t + \mathcal{N}(0,{\sigma_t}^2)gt =gt
+N(0,σt 2)

This updated gradient is used in backpropagation. Instead of fixed variance, [8]
recommends the use of decaying variance (also called annealing Gaussian noise):

σt2=η(1+t)γ{\sigma_t}^2 = \frac{\eta}{(1 + t)^{\gamma}}σt 2=(1+t)γη

In [8], η\etaη is chosen from the set {0.01, 0.3, 1.0} and γ\gammaγ is set to
0.55. Let us see how the variance varies with an increase in t. The variance of
the noise added decreases as the training proceeds.

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

t = np.linspace(1,100)
y = np.zeros((len(t),3))
eta = [0.01,0.3,1]
gamma = 0.55

for idx,eta in enumerate(eta):
  y[:,idx] = eta/np.power(1+t,gamma)

df = pd.DataFrame({'t':t,
                   'η = 0.01': y[:,0],
                   'η = 0.3': y[:,1],
                   'η = 1': y[:,2]
                   })

df = df.melt('t',var_name="η values",value_name="noise variance")

sns.lineplot(data=df,x="t",y="noise variance",hue="η values")
plt.title("Annealing Gaussian Noise")


Annealing Gaussian Noise
[https://cdn.sanity.io/images/vr8gru94/production/c0d5644cfb476a51febd6984bc50079bbb5a5313-393x284.png]



DROPOUT

Dropout is another popular regularization technique. To understand how dropout
works, it’ll help to review the concept of model ensembling.


REVISITING ENSEMBLE METHODS

In traditional machine learning, model ensembling helps reduce overfitting and
improve model performance. For a simple classification problem, we can take one
of the following approaches:

 1. Train multiple classifiers to solve the same task.
 2. Train different instances of the same classifier for different subsets of
    the training dataset.

For a simple classification model, ensemble technique such as bagging involves
training the same classifier—on different subsets of training data—sampled with
replacement. Suppose there are N such instances. At test time, the test sample
is run through each classifier, and an ensemble of their predictions is used.

In general, the performance of an ensemble is at least as good as the individual
models; it cannot be worse than that of the individual models.

If we were to transpose this idea to neural networks, we could try doing the
following (while identifying the limitations of this approach):

 * Train multiple neural networks with different architectures. Train a neural
   network on different subsets of the training data. However, training multiple
   neural networks is prohibitively expensive.
 * Even if we train N different neural networks, running the data point through
   each of the N models—at test time—introduces substantial computational
   overhead.

Dropout is a regularization technique that addresses both of the above concerns.


HOW DROPOUT WORKS

Let’s consider a simple neural network:

A Simple Neural Network
[https://cdn.sanity.io/images/vr8gru94/production/d60dc9fb06520e0d3dfb05b9ba61c7decd462ac1-1000x1037.png]
A Simple Neural Network

Dropout involves dropping neurons in the hidden layers and (optionally) the
input layer. During training, each neuron is assigned a “dropout”probability,
like 0.5.

With a dropout of 0.5, there’s a 50% chance of each neuron participating in
training within each training batch. This results in a slightly different
network architecture for each batch. It is equivalent to training different
neural networks on different subsets of the training data.

Dropout
[https://cdn.sanity.io/images/vr8gru94/production/8ab3bc8421242b66949f85c4eca3a7e3eca66dd2-1000x1037.png]
Dropout

The weight matrix is initialized once at the beginning of the training. In
general, for the k-th batch, backpropagation occurs only along those paths only
through the neurons present for that batch. Meaning only the weights
corresponding to neurons that are present get updates.

At test time, all the neurons are present in the network. So how do we account
for the dropout during training? We weight each neuron’s output by the same
probability p – proportional to the fraction of time the neuron was present
during the training.


SUMMING UP

Regularization techniques aim to facilitate better generalization by minimizing
complexity. To summarize the different techniques covered:

 * Early stopping reduces overfitting by stopping the training error from
   becoming too low consistently.
 * Data augmentation helps the model generalize better by exposing the model to
   diverse training examples during training. For image data, you can apply
   simple label-invariant transformations or newer techniques such as MixUp,
   AugMix, and CutMix.
 * In general, Lp norms penalize large weights. L1 regularization promotes
   sparsity by forcing many weights to go to zero. L2 regularization clips the
   weights from becoming too large while ensuring that the important components
   in the weight vector are larger than the other components.
 * We can add noise to the inputs, outputs, or gradients of the network to
   prevent a neural network from overfitting on the training data.
 * Dropout helps us emulate model ensembles by dropping out neurons during the
   training process with a fixed probability p, ensuring that a different neural
   network architecture is used for different batches during training. We weight
   a neuron’s output at test time by the same probability p.

In addition to these common regularization techniques, you can apply and use
techniques to improve the training of neural networks.


REFERENCES

[1] R Wu et al., , arXiv, 2015

[2] H. Zhang, M. Cisse, Y. N.Dauphin, D. Lopez-paz, , arXiv, 2017

[3] T. DeVries et al., , arXiv, 2017

[4] S. Yun et al., , ICCV 2019

[5] D. Hendryks, N. Mu et al., , ICLR 2020

[6] CM Bishop, , Neural Computation, 1995

[7] L. Xie, J. Wang, Z.Wei, M. Wang, Q. Tian, (2016), CVPR 2016

[8] A. Neelakantan et al., , arXiv, 2015

[9] Goodfellow et al., , MIT Press, 2016

Share via:


FURTHER READING


Author
[/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&w=3840&q=100]

Bala Priya C

Technical Writer

--------------------------------------------------------------------------------

Jump to section
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 

Share via:


Product

Solutions

Resources

Company

Legal


© Pinecone Systems, Inc. | San Francisco, CA

Pinecone is a registered trademark of Pinecone Systems, Inc.

[/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpopup-bg.197eeb2c.png&w=3840&q=100]


INTRODUCING — PINECONE SERVERLESS

Build knowledgeable AI at up to 50x lower cost. No need to manage
infrastructure. Get started with $100 in usage credits.



DON'T MISS THE NEXT ONE...



Get an email the next time we publish an article about machine learning and
similarity search.

Get Updates