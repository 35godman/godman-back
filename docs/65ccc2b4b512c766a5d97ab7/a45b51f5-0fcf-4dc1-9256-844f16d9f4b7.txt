Opens in a new window Opens an external website Opens an external website in a
new window
<!---->Close this dialog<!---->
This website utilizes technologies such as cookies to enable essential site
functionality, as well as for analytics, personalization, and targeted
advertising purposes. To learn more, view the following link:



<!---->Close Cookie Preferences<!---->


Product
Solutions
Resources
Company



BUILDING THE SELF-ORGANIZING WORKSPACE AT MEM

--------------------------------------------------------------------------------

Written by  for . Reposted with permission.

Over the course of our lives, we spend a vast amount of time creating and
capturing information. Yet we lack the ability to usefully draw from this well
of knowledge, as it often becomes lost in folders or information silos.

At , we are building a world in which every person has access to the information
they need when they need it. We leverage AI technology to create a
self-organizing workspace that automatically organizes all of the information in
your work life and proactively surfaces relevant knowledge.

Our long-term mission is to unlock the collective intelligence of humanity. To
realize our vision for the future, we are harnessing a technological inflection
point: the quality of publicly available foundation models.

Recent breakthroughs in large language models (LLMs) like GPT-3 have drastically
changed the field of Natural Language Processing (NLP). Unlike previous
generations of NLP that required the construction of separate models for each
specific language task, these LLMs are not specialized for any particular task.
With a small amount of fine-tuning, we have been able to optimize these
pre-trained LLMs for our own use cases.

Most recently, creators of LLMs have also started to open up their black boxes,
releasing access to internal layers from within those models. , for example,
allow users to access the embedding layers that encode a text’s meaning, giving
more insight into the fundamental building blocks of their NLP AI than the
direct GPT-3 output alone can provide. are high-dimensional vectors that encode
different features of text documents, including meaning, structure, content,
theme and topic. Texts with similar meanings will have similar vector
representations, and by comparing embeddings of different pieces of text, we can
measure the similarity between them. Embeddings make natural language tasks such
as and clustering of similar documents easy to perform.

The ability to carry out these similarity calculations at query time is critical
when building products that rely on embeddings. is a leader in the vector search
space, and their vector database allows users to store embeddings and quickly
query for similar embeddings based on various similarity measures and filters.

We leverage both OpenAI embeddings models and Pinecone as fundamental pillars of
Mem X. These technologies power features such as similar mems and smart results,
among others. Similar mems surfaces documents that are semantically similar to
the document a user is viewing, allowing users to discover knowledge from across
their team, re-discover knowledge they forgot they had, and make new connections
between pieces of information they might not have otherwise seen. Smart results
allows users to ask Mem questions as though it were a person – e.g., “How many
people did we add to the Mem X waitlist in March?”. With smart results, Mem
understands the semantic meaning of a user’s search query and then finds the
most relevant results.

OpenAI offers different embeddings models specialized for different
functionalities. We use the text similarity and text search models. The
similarity embeddings are good at capturing semantic similarity between multiple
pieces of text, and the text search embeddings are trained to measure whether
long documents are relevant to a short search query.

Mem X overview
[https://cdn.sanity.io/images/vr8gru94/production/4a6479b78eaeae7da3a3bd1f03c512a832903e10-800x428.png]


We transform each document into a format that can be embedded, and use OpenAI’s
embeddings API to create two embeddings for the document, one with a similarity
model and the other with a search model. The embeddings are stored in a Pinecone
index, along with metadata about the document. We leverage Pinecone’s namespaces
to create divisions between vectors that are produced by different models. As a
user edits a document, we continuously re-compute the embeddings for this
document and upsert the new embeddings to the Pinecone index to ensure that our
embeddings are always up to date.

Smart results Pinecone index
[https://cdn.sanity.io/images/vr8gru94/production/834ae9c5c7aa6cacabd0cea41c684df079744d00-800x438.png]


In the case of smart results, when a user makes a search, we parse and transform
the search query before creating an embedding with one of OpenAI’s search query
models, and then query the Pinecone index to find the most similar search
documents (i.e. documents with the highest cosine similarity score). Pinecone’s
metadata filtering functionality allows us to query for only those embeddings
that represent documents to which the currently signed-in user has access. We
then reconcile the search results returned from Pinecone with our non-semantic
search service to improve keyword results, and display the documents
corresponding to these embeddings.

Similar mems Pinecone index
[https://cdn.sanity.io/images/vr8gru94/production/10ecf953faa4ecc7371f2ef50dc15247d034988a-800x454.png]


In the similar mems feature, when a user views a document, we fetch the
embedding for the document from the Pinecone index, then query the index for the
most similar embeddings according to metadata filters. We re-rank and re-weight
these similar embeddings based on our own clustering and length normalization
algorithms, and surface the documents that the embeddings most closely
correspond to.

Over time, we will be able to automatically organize all of the information that
exists within an organization, from employee information to customer data,
internal documents, research, emails, Slack messages, and more.

Learn more about Mem:

 * 
 * 

Share via:


Take a look at the hidden world of vector search and its incredible potential.
(series cover image)
[/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fdfd58b57af6542487ba4145e668f633aaea245d6-1700x2192.png&w=3840&q=100]

Chapters

 1. 
 2. 
 3. 
 4. 


Product

Solutions

Resources

Company

Legal


© Pinecone Systems, Inc. | San Francisco, CA

Pinecone is a registered trademark of Pinecone Systems, Inc.

[/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpopup-bg.197eeb2c.png&w=3840&q=100]


INTRODUCING — PINECONE SERVERLESS

Build knowledgeable AI at up to 50x lower cost. No need to manage
infrastructure. Get started with $100 in usage credits.



DON'T MISS THE NEXT ONE...



Get an email the next time we publish an article about machine learning and
similarity search.

Get Updates