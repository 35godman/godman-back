Opens in a new window Opens an external website Opens an external website in a
new window
<!---->Close this dialog<!---->
This website utilizes technologies such as cookies to enable essential site
functionality, as well as for analytics, personalization, and targeted
advertising purposes. To learn more, view the following link:



<!---->Close Cookie Preferences<!---->


Product
Solutions
Resources
Company



BAG OF VISUAL WORDS

--------------------------------------------------------------------------------

Jump to section
 * 
 * 
 * 

--------------------------------------------------------------------------------

In computer vision, bag of visual words (BoVW) is one of the pre-deep learning
methods used for building image embeddings. We can use BoVW for content-based
image retrieval, object detection, and image classification.

At a high level, comparing images with the bag of visual words approach consists
of five steps:

 1. Extract visual features,
 2. Create visual words,
 3. Build sparse frequency vectors with these visual words,
 4. Adjust frequency vectors for relevant with tf-idf,
 5. Compare vectors with similarity or distance metrics.

We will start by walking through the theory of how all of this works. In the
second half of this article we will look at how to implement all of this in
Python.




HOW BAG OF VISUAL WORDS WORKS


VISUAL FEATURES

The model derives from bag of words in natural language processing (NLP), where
a chunk of text is split into words or sub-words and those components are
collated into an unordered list, the so-called “bag of words” (BoW).

Bag of visual words drawing
[https://cdn.sanity.io/images/vr8gru94/production/9ab579780145c3afe3fd9a7c6050a5b520f92631-653x813.png]


Similarly, in bag of visual words the images are represented by patches, and
their unique patterns (or visual features) are extracted from the image.

Visual feature extraction
[https://cdn.sanity.io/images/vr8gru94/production/aad6b6f22fcfc9392bb5cb3d3dbc21f6a2fabdcc-1290x987.png]


However, despite the similarity, these visual features are not visual words just
yet; we must perform a few more steps. For now, let’s focus on understanding
what these visual features are.

Visual features consist of two items:

 * Keypoints are points in an image, which do not change if the image is
   rotated, expanded, or scaled and
 * Descriptors are vector representations of an image patch found at a given
   keypoint.

These visual features can be detected and extracted using a feature detector,
such as SIFT (Scale Invariant Feature Transform), ORB (Oriented FAST and Rotated
BRIEF), or SURF (Speeded Up Robust Features).

The most common is SIFT as it is invariant to scale, rotation, translation,
illumination, and blur. SIFT converts each image patch into a 128128-dimensional
vector (i.e., the descriptor of the visual feature).

A single image will be represented by many SIFT vectors. The order of these
vectors is not important, only their presence within an image.


CODEBOOKS AND VISUAL WORDS

After extracting visual features we build a codebook, also called a dictionary
or vocabulary. This codebook acts as a repository of all existing visual words
(similar to an actual dictionary, like the Oxford English Dictionary).

We use this codebook as a way to translate a potentially infinite variation of
visual features into a predefined set of visual words.

How? The idea is to group similar visual features into clusters. Each cluster is
assigned a central point which represents the visual word translation (or
mapping) for that group of visual features. The standard approach for grouping
visual features into visual words is .

K-means divides the data into kkk clusters, where kkk is chosen by us. Once the
data is grouped, k-means calculates the mean for each cluster, i.e., a central
point between all of the vectors in a group. That central point is a centroid
(i.e., a visual word).

Centroid
[https://cdn.sanity.io/images/vr8gru94/production/420b0992c5de275e0defe9d0d2a3fa7ee3f661a8-986x325.png]


After finding the centroids, k-means iterates through each data point (visual
feature) and checks which centroid (visual word) is nearest. If the nearest
centroid has changed, the data point switches grouping, being assigned to the
new nearest centroid.

This process is repeated over a given number of iterations or until the centroid
positions have stabilized.

With that in mind, how do we choose the number of centroids, kkk?

It is more of an art than a science, but there are a few things to consider.
Primarily, how many visual words can cover the various relevant visual features
in the dataset.

That’s not an easy thing to figure out, and it’s always going to require some
guesswork. However, we can think of it using the language equivalent, bag of
words.

If our language dataset covered several documents about a specific topic in a
single language, we would find fewer unique words than if we had thousands of
documents, spanning several languages about a range of topics.

The same is true for images; dogs and/or animals could be a topic, and buildings
could be another topic. As for the equivalent of different languages, this is
not a perfect metaphor but we could think of different photography styles,
drawings, or cartoons. All of these added layers of complexity increase the
number of visual words needed to accurately represent the dataset.

Here, we could start with choosing a smaller kkk value (e.g., 100100 or 150150)
and re-run the code multiple times changing kkk until convergence and/or our
model seems to be identifying images well.

If we choose kkk=150=150, k-means will generate 150150 centroids and, therefore,
150150 visual words.

When we perform the mapping from new visual feature vectors to the nearest
centroid (i.e., visual word), we categorize visual features into a more limited
set of visual words. This process of reducing the number of possible unique
vectors is called vector quantization.

Vector quantization
[https://cdn.sanity.io/images/vr8gru94/production/15f8aa37b87b5f3d6912583ab66e676992dbdf4e-873x907.png]


Using a limited set of visual words allows us to compress our image descriptions
into a set of visual word IDs. And, more importantly, it helps us represent
similar features across images using a shared set of visual words.

That means that the visual words shared by two images of churches may be quite
large, meaning they’re similar. However, an image of a church and an image of a
dog will share far fewer visual words, meaning they’re dissimilar.

After those steps, our images will be represented by a varying number of visual
words. From here we move on to the next step of transforming these visual words
into image-level frequency vectors.


FREQUENCY VECTORS

We can count the frequency of these visual words and visualize them with
histograms.

The x-axis of the histogram is the codebook, while the y-axis is the frequency
of each visual word (in the codebook) for that image.

If we consider 22 images, we can represent the image histograms as follows:

Frequency vectors
[https://cdn.sanity.io/images/vr8gru94/production/fa23010abe2afa514a2874d10c0da6ecab52e886-908x840.png]


To create these representations, we have converted each image into a sparse
vector where each value in the vector represents an item in the codebook (i.e.,
the x-axis in the histograms). Most of the values in each vector will be zero
because most images will only contain a small percentage of total number of
visual words, which is why we refer to them as sparse vectors.

As for the non-zero values in our sparse vector, they are calculated in the same
way that we calculated our histogram bar heights. They are equal to the
frequency of a particular visual word in an image.

This works, but it’s a crude way to create these sparse vector representations.
Because many visual words are actually not that important, we add one more step.


TF-IDF

In language there are some words that are more important than others in that
they give us more information. If we used the sentence “the history of Rome” to
search through a set of articles, the words “the” and “of” should not be given
the same importance as “history” or “Rome”.

These less important words are often very common. If we only consider the
frequency of words shared with our “the history of Rome” query, the article with
the most “the"s could be scored highest.

This problem is also found in images. A visual word extracted from a patch of
sky in an image is unlikely to tell us whether this image is of a church or a
dog. Some visual words are more relevant than others.

tf-idf
[https://cdn.sanity.io/images/vr8gru94/production/08b9896de6e3cadda622f490f7fc0697c6857d5d-873x645.png]


In the example above, we would expect a visual word representing the sky 1 to be
less relevant than a visual word representing the cross on top of the bell tower
2.

That is why it is important to adjust the values of our sparse vector to give
more weight to more relevant visual words and less weight to less relevant
visual words.

To do that, we can use the tf-idf (term-frequency inverse document frequency)
formula, which is calculated as follows:

tf–idft,d=tft,d∗idft=tft,d∗logNdfttf\textrm{--}idf_{t,d} = tf_{t,d} * idf_t =
tf_{t,d} * log\frac{N}{df_t}tf–idft,d =tft,d ∗idft =tft,d ∗logdft N

Where:

 * tft,dtf_{t,d}tft,d is the term frequency of the visual word ttt in the image
   ddd (the number of times ttt occurs in ddd),
 * NNN is the total number of images,
 * dftdf_tdft number of images containing visual word ttt,
 * logNdftlog\frac{N}{df_t}logdft N measures how common the visual word ttt is
   across all images in the database. This is low if the visual word ddd occurs
   many times in the image, high otherwise.

After tf-idf, we can visualize the vectors via our histogram again, which will
better reflect the image’s features.

visualization via histogram
[https://cdn.sanity.io/images/vr8gru94/production/086e91272df8d280a292ad505bca67b93c8d76ad-908x840.png]


Before we were giving the same importance to image’s patches in an image; now
they’re adjusted based on relevance and then normalized.

We’ve now trained our codebook and learned how to process each vector for better
relevance and normalization. When wanting to embed new images with this
pipeline, we repeat the process but avoid retraining the codebook. Meaning we:

 1. Extract the visual features,
 2. Transform them into visual words using the existing codebook,
 3. Use these visual words to create a sparse frequency vector,
 4. Adjust the frequency vector based on relevance with tf-idf, giving us our
    final sparse vector representations.

After that, we’re ready to compare these sparse vectors to find similar or
dissimilar images.


MEASURING SIMILARITY

There are several metrics we can use to calculate similarity or distance between
two vectors. The most common are:

 1. Cosine similarity,
 2. Euclidean distance, and
 3. Dot product similarity.

We will use cosine similarity which measures the angle between vectors. Vectors
pointing in a similar direction have a lower angular separation and therefore
higher cosine similarity.

Cosine similarity is calculated as:

cossim(A,B)=cos(θ)=A⋅B∣∣A∣∣ ∣∣B∣∣cossim(A,B)= cos(\theta)=\frac{A \cdot B}{||A||
\space ||B||}cossim(A,B)=cos(θ)=∣∣A∣∣ ∣∣B∣∣A⋅B

Cosine similarity generally gives a value ranging [−1,1]. However, if we think
about the frequency of visual words, we cannot consider them as negative.
Therefore, the angle between two term frequency vectors cannot be greater than
90°, and cosine similarity ranges between [0,1].

It equals 1 if the vectors are pointing in the same direction (the angle equals
0) and 0 if vectors are perpendicular.

Cosine similarity diagram
[https://cdn.sanity.io/images/vr8gru94/production/faeebc6dc3eb90a63a60a474bf29f008cc9eb3c8-2666x1140.png]


If we consider three different images and we build a matrix based on cosine
similarity:

Cosine similarity matrix
[https://cdn.sanity.io/images/vr8gru94/production/66ac199d7956ddaf62e57fe73b0d0ed7f8d41606-1276x673.png]


We can see that cosine similarity is 11 when the image is exactly the same
(i.e., in the main diagonal). The cosine similarity approaches 00 as the images
have less in common.

Let’s now move on to implementing bag of visual words with Python.


IMPLEMENTING BAG OF VISUAL WORDS

The next section will work through the implementation of everything we’ve just
learned in Python. If you’d like to follow along, use .


IMAGENETTE DATASET PREPROCESSING

First, we want to import a dataset of images to train the model.

Feel free to use any images you like. However, if you’d like to follow along
with the same dataset, we will use the frgfm/imagenette dataset from HuggingFace
Datasets.

In[2]:

from datasets import load_dataset

# download the dataset
imagenet = load_dataset(
    'frgfm/imagenette',
    'full_size',
    split='train',
    ignore_verifications=False  # set to True if seeing splits Error
)
imagenet


Out[2]:

Dataset({
    features: ['image', 'label'],
    num_rows: 9469
})

The dataset contains 9469 images, covering a range of images with dogs, radios,
fishing, cities, etc. The image feature contains the images themselves stored as
PIL object, meaning we can view them in a notebook like so:

# important to use imagenet[0]['image'] rather than imagenet['image'][0]
# as the latter loads the entire image column then extracts index 0 (it's slow)
imagenet[3264]['image']


Preprocessing an image
[https://cdn.sanity.io/images/vr8gru94/production/6735055fb98e0156f3ea25eaef3afe8b1bb7000e-500x365.png]


imagenet[5874]['image']


Preprocessing Image of a dog
[https://cdn.sanity.io/images/vr8gru94/production/2d210106d1cb697dd837741364ffecb8a7f07f46-500x375.png]


To process these images we need to transform them from PIL objects to numpy
arrays.

import numpy as np

# initialize list
images_training = []

for n in range(0,len(imagenet)):
    # generate np arrays from the dataset images
    images_training.append(np.array(imagenet[n]['image']))



The dataset mostly consists of color images containing three color channels
(red, green, and blue), but some are also grayscale containing just a single
channel (brightness). To optimize processing time and keep everything as simple
as possible, we will transform color images to grayscale.

import cv2  # pip install opencv-contrib-python opencv-python

# convert images to grayscale
bw_images = []
for img in images_training:
    # if RGB, transform into grayscale
    if len(img.shape) == 3:
        bw_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))
    else:
        # if grayscale, do not transform
        bw_images.append(img)



The arrays in bw_images are what we will be using to create our visual features,
visual words, frequency vectors, and tf-idf vectors.


VISUAL FEATURES

With our dataset prepared we’re ready to move on to extracting visual features
(both keypoints and descriptors). As mentioned earlier, we will use the SIFT
feature detection algorithm.

# defining feature extractor that we want to use (SIFT)
extractor = cv2.xfeatures2d.SIFT_create()

# initialize lists where we will store *all* keypoints and descriptors
keypoints = []
descriptors = []

for img in bw_images:
    # extract keypoints and descriptors for each image
    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)
    keypoints.append(img_keypoints)
    descriptors.append(img_descriptors)



It’s worth noting that if an image doesn’t have any noticeable features (e.g.,
it is a flat image without any edges, gradients, etc.), extraction with SIFT can
return None. We don’t have that problem with this dataset, but it’s something to
watch out for with others.

Now that we have extracted the visual features, we can visualize them with
matplotlib.

output_image = []
for x in range(3):
    output_image.append(cv2.drawKeypoints(bw_images[x], keypoints[x], 0, (255, 0, 0),
                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))
    plt.imshow(output_image[x], cmap='gray')
    plt.show()


Visual features
[https://cdn.sanity.io/images/vr8gru94/production/8f3711203e62ee36d13a5ec05d41f60c72cdeb58-375x223.png]

Visual features
[https://cdn.sanity.io/images/vr8gru94/production/d8f6dac4365319848d059ca1c7daae4e956320f3-330x252.png]

Visual features
[https://cdn.sanity.io/images/vr8gru94/production/118fdfb7a16324d42b7704896970965eb138de0b-327x252.png]


The centre of each circle is the keypoint location, and the lines from the
centre of each circle represent keypoint orientation. The size of each circle is
the scale at which the features were detected.

With our visual features ready, we can move onto the next step of creating
visual words.


VISUAL WORDS AND THE CODEBOOK

Earlier we described the “codebook”. The codebook acts as a vocabulary where we
store all of our visual words. To create the codebook we use k-means clustering
to quantize our visual features into a smaller set of visual words.

Our full set of visual features is big, and training k-means with the full set
will take some time. So, to avoid that and also emulate a real-world scenario
where we are unlikely to train on all images that we’ll ever process, we will
use a smaller sample of 1000 images.

# set numpy seed for reproducability
np.random.seed(0)
# select 1000 random image index values
sample_idx = np.random.randint(0, len(imagenet)+1, 1000).tolist()

# extract the sample from descriptors
# (we don't need keypoints)
descriptors_sample = []

for n in sample_idx:
    descriptors_sample.append(np.array(descriptors[n]))



Our descriptors_sample contains a single array for each image, and each array
can contain a varying number of SIFT feature vectors. When training k-means, we
only care about the feature vectors, we don’t care about which image they’re
coming from. So, we need to flatten descriptors_sample into a single array
containing all descriptors.

In[13]:

all_descriptors = []
# extract image descriptor lists
for img_descriptors in descriptors_sample:
    # extract specific descriptors within the image
    for descriptor in img_descriptors:
        all_descriptors.append(descriptor)
# convert to single numpy array
all_descriptors = np.stack(all_descriptors)


In[14]:

# check the shape 
all_descriptors.shape


Out[14]:

(1130932, 128)

From this, we get all_descriptors, a single array containing all feature vectors
from our sample. There are ~1.3M of these.

We now want to group similar visual features (descriptors) using k-means. After
a few tests, we chose k=200 for our model.

After k-means, all images will have been reduced to visual words, and the full
set of these visual words become our codebook.

# perform k-means clustering to build the codebook
from scipy.cluster.vq import kmeans

k = 200
iters = 1
codebook, variance = kmeans(all_descriptors, k, iters)



Once built, the codebook does not change. No matter how many more visual
features we process, no more visual words are added as we will use it solely as
a mapping between new visual features and the existing visual words.

--------------------------------------------------------------------------------

It can be difficult to find the optimal size of our codebook - if too small,
visual words could be unrepresentative of all image regions, and if too large,
there could be too many visual words with little to no of them being shared
between images (making comparisons very hard or impossible).

--------------------------------------------------------------------------------

With our codebook complete, we can use it to transform the full dataset of
visual features into visual words.

In[19]:

# vector quantization
from scipy.cluster.vq import vq

visual_words = []
for img_descriptors in descriptors:
    # for each image, map each descriptor to the nearest codebook entry
    img_visual_words, distance = vq(img_descriptors, codebook)
    visual_words.append(img_visual_words)


In[20]:

# let's see what the visual words look like for image 0
visual_words[0][:5], len(visual_words[0])


Out[20]:

(array([ 84,  22,  45, 172, 172], dtype=int32), 397)

In[21]:

# the centroid that represents visual word 84 is of dimensionality...
codebook[84].shape  # (all have the same dimensionality)


Out[21]:

(128,)

We can see here that image 0 contains 397 visual words; the first five of those
are represented by [84, 22, 45, 172, 172], which are the index values of the
visual word vector found in the codebook. This visual word vector shares the
same dimensionality as our SIFT feature vectors because it represents a cluster
centroid from those feature vectors.


SPARSE FREQUENCY VECTORS

After building our codebook and creating our image representations with visual
words, we can move on to building sparse vector representations from these
visual words.

We do this to compress the many visual word vectors representing our images into
a single vector of set dimensionality. By doing this, we are able to directly
compare our image representations using metrics like cosine similarity and
Euclidean distance.

To create these frequency vectors, we look at how many times each visual word is
found in an image. There are only 200 unique visual words (the length of our
codebook), so each of these frequency vectors will have dimensionality 200,
where each value becomes a count for a specific visual word.

In[22]:

frequency_vectors = []
for img_visual_words in visual_words:
    # create a frequency vector for each image
    img_frequency_vector = np.zeros(k)
    for word in img_visual_words:
        img_frequency_vector[word] += 1
    frequency_vectors.append(img_frequency_vector)
# stack together in numpy array
frequency_vectors = np.stack(frequency_vectors)


In[23]:

frequency_vectors.shape


Out[23]:

(9469, 200)

After creating the frequency vectors, we’re left with a single vector
representation for each image. We can see an example of the frequency vector for
image 0 below:

In[45]:

# we know from above that ids 84, 22, 45, and 172 appear in image 0
for i in [84,  22,  45, 172]:
    print(f"{i}: {frequency_vectors[0][i]}")


Out[45]:

84: 2.0
22: 2.0
45: 3.0
172: 4.0


In[25]:

frequency_vectors[0][:20]


Out[25]:

array([0., 0., 2., 0., 2., 1., 0., 3., 1., 0., 0., 0., 5., 3., 4., 0., 1.,
       0., 0., 0.])

In[26]:

# visualize the frequency vector for image 0
plt.bar(list(range(k)), frequency_vectors[0])
plt.show()


Out[26]:

<Figure size 432x288 with 1 Axes>[data:image/png;base64,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]


TF-IDF

Our frequency vector can already be used for comparing images using our
similarity and distance metrics. However, it is not ideal as it does not
consider the different levels of relevance of each visual word. So, we must use
tf-idf to adjust the frequency vector to consider relevance.

tf-idft,d=tft,d∗idft=tft,d∗logNdfttf\textrm{-}idf_{t,d} = tf_{t,d} * idf_t =
tf_{t,d} * log\frac{N}{df_t}tf-idft,d =tft,d ∗idft =tft,d ∗logdft N

We first calculate NNN and dftdf_tdft , both of which are shared across the
entire dataset as the image ddd is not considered by either parameter.
Naturally, idftidf_tidft also produces a single vector shared by the full
dataset.

In[27]:

# N is the number of images, i.e. the size of the dataset
N = 9469

# df is the number of images that a visual word appears in
# we calculate it by counting non-zero values as 1 and summing
df = np.sum(frequency_vectors > 0, axis=0)


In[28]:

df.shape, df[:5]


Out[28]:

((200,), array([7935, 7373, 7869, 8106, 7320]))

In[29]:

idf = np.log(N/ df)
idf.shape, idf[:5]


Out[29]:

((200,), array([0.17673995, 0.25019863, 0.18509232, 0.15541878, 0.25741298]))

With idftidf_tidft calculated, we just need to multiply it by each
tft,dtf_{t,d}tft,d vector to get our tf-idft,dtf \textrm{-} idf_{t,d}tf-idft,d
vectors. Fortunately, we already have the tft,dtf_{t,d}tft,d vectors, as they
are our frequency vectors.

In[30]:

tfidf = frequency_vectors * idf
tfidf.shape, tfidf[0][:5]


Out[30]:

((9469, 200),
 array([0.        , 0.        , 0.37018463, 0.        , 0.51482595]))

In[31]:

# we can visualize the histogram for image 0 again
plt.bar(list(range(k)), tfidf[0])
plt.show()


Out[31]:

<Figure size 432x288 with 1 Axes>[data:image/png;base64,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]

We now have 9469 200-dimensional sparse vector representations of our images.


SEARCH

These sparse vectors have been built in such a way that images that share many
similar visual features should share similar sparse vectors. We can use cosine
similarity to compare these images and identify similar images.

We will start by searching with image 1200:

Image 1200
[https://cdn.sanity.io/images/vr8gru94/production/5ab3d35085ec1f75f9cbc20862c6ce39af7a4989-1428x952.png]


from numpy.linalg import norm

top_k = 5
i = 1200

# get search image vector
a = tfidf[i]
b = tfidf  # set search space to the full sample
# get the cosine distance for the search image `a`
cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))
# get the top k indices for most similar vecs
idx = np.argsort(-cosine_similarity)[:top_k]
# display the results
for i in idx:
    print(f"{i}: {round(cosine_similarity[i], 4)}")
    plt.imshow(bw_images[i], cmap='gray')
    plt.show()



1200: 1.0

1200: 1.0
[https://cdn.sanity.io/images/vr8gru94/production/89b000259baa33f3a36bc5edbefb2472cbabffb4-367x252.png]


1609: 0.944

1609: 0.944
[https://cdn.sanity.io/images/vr8gru94/production/151c653d521b02bbba91bc4d709e99e72bbc7d37-367x252.png]


1739: 0.9397

1739: 0.9397
[https://cdn.sanity.io/images/vr8gru94/production/3adf7f3307f7834161bdb8faf6ed8299b0af0321-330x252.png]


4722: 0.9346

4722: 0.9346
[https://cdn.sanity.io/images/vr8gru94/production/85406dc29580c8d9b2311a18f94c6ae400ca057c-330x252.png]


7381: 0.9304

7381: 0.9304
[https://cdn.sanity.io/images/vr8gru94/production/d87c36d2f01b90320407b86ceed4bd1616554497-330x252.png]


The top image is of course the same image; as they are exact matches we can see
the expected cosine similarity score of 1.0. Following this, we have two highly
similar results. Interestingly, the fourth image seems to have been pulled
through due to similarity in background foliage.

Here are a few more sets of results, showing the varied performance of the
approach with different items.

Results
[https://cdn.sanity.io/images/vr8gru94/production/11028019c3d3516ce047bed0712e1ea56731bbdd-1316x811.png]


Here we get a good first result, followed by irrelevant images and a final good
result in fifth position, a 50% success rate.

Good first result
[https://cdn.sanity.io/images/vr8gru94/production/e49e70518c1d472f6409f7d4d0db123935715ebd-1775x831.png]


Bag of visual words seems to work well with golf balls, identifying 3/4 of
relevant images.

If you’re interested in seeing more results, check out the .

--------------------------------------------------------------------------------

That’s it for this article on bag of visual words, one of the most successful
methods for image classification and retrieval without the use of neural
networks or other deep learning methods.

One great aspect of this approach is that it is fairly reliable and
interpretable. There is no black box of AI here, so when applied to a lot of
data we will rarely get too many surprising results.

We know that images with similar edges, textures, and colors are likely to be
identified as similar; the features being identified are set by the SIFT (or
other) algorithms.

All of this makes bag of visual words a good option for image retrieval or
classification, where we need to focus on the features that we know algorithms
like SIFT can deal with, i.e. we’re focused on finding similar object edges
(that are resistant to scale, noise, and illumination changes). Finally, these
well-defined algorithms give us a huge advantage when interpretability is
important.


RESOURCES

, GitHub

Share via:


Learn about the past, present, and future of image search, text-to-image, and
more. (series cover image)
[/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9952912500cbe79a9729a37e31b74ac940f0cb7b-850x1096.png&w=3840&q=100]

Chapters

 1. 
 2.  * 
     * 
     * 

 3. 
 4. 
 5. 
 6. 
 7. 
 8. 


Product

Solutions

Resources

Company

Legal


© Pinecone Systems, Inc. | San Francisco, CA

Pinecone is a registered trademark of Pinecone Systems, Inc.

[/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpopup-bg.197eeb2c.png&w=3840&q=100]


INTRODUCING — PINECONE SERVERLESS

Build knowledgeable AI at up to 50x lower cost. No need to manage
infrastructure. Get started with $100 in usage credits.



DON'T MISS THE NEXT ONE...



Get an email the next time we publish an article about machine learning and
similarity search.

Get Updates