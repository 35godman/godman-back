Opens in a new window Opens an external website Opens an external website in a
new window
<!---->Close this dialog<!---->
This website utilizes technologies such as cookies to enable essential site
functionality, as well as for analytics, personalization, and targeted
advertising purposes. To learn more, view the following link:



<!---->Close Cookie Preferences<!---->


Product
Solutions
Resources
Company



WEIGHT INITIALIZATION TECHNIQUES IN NEURAL NETWORKS

Jun 30, 2023

Author
[/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&w=3840&q=100]

Bala Priya C

Technical Writer

--------------------------------------------------------------------------------

Jump to section

--------------------------------------------------------------------------------

Weight Initialization
[https://cdn.sanity.io/images/vr8gru94/production/9cc3fd6b98fbc0bbf6bb50d58d226eea7dacec7b-1000x558.png]


You can build better deep learning models that train much faster by using the
correct weight initialization techniques. A neural network learns the weights
during the training process. But how much do the initial weights of the network
benefit or hinder the optimization process?

Though the neural network “learns” the optimal values for the weights, the
initial values of the weights play a significant role in how quickly and to
which local optimum the weights converge.

Initial weights have this impact because the loss surface of a deep neural
network is a complex, high-dimensional, and with many local minima. So the point
where the weights start on this loss surface determines the local minimum to
which they converge; the better the initialization, the better the model.

This tutorial will discuss the early approaches to weight initialization and the
limitations of zero, constant, and random initializations. We’ll then learn
better weight initialization strategies based on the number of neurons in each
layer, choice of activation functions, and more.

Let’s begin!


EARLY APPROACHES TO WEIGHT INITIALIZATION

When training deep neural networks, finding the optimal initial value for
weights was one of the earliest challenges faced by deep learning researchers.
In 2006, Geoffrey Hinton and Salakhutdinov introduced a weight initialization
strategy called Greedy Layerwise Unsupervised Pretraining [1]. To parse the
algorithm’s definition, let’s understand how it works.

Given the input layer and the first hidden layer, an unsupervised learning
model, such as an autoencoder, is used to learn the weights between the input
and the first hidden layer.

Pretraining the first hidden layer
[https://cdn.sanity.io/images/vr8gru94/production/c29d99fb852c52399f333c78638341431215ad64-1000x563.png]
Learning the initial weights for the first hidden layer (Image by the author)

These weights are frozen and are used as inputs to learn the weights that flow
into the next hidden layer.

Pretraining the second hidden layer
[https://cdn.sanity.io/images/vr8gru94/production/30ac0e85d2dbc694799b6372bdea47abc6279df0-1000x563.png]
Learning the initial weights for the second hidden layer (Image by the author)

The process continues until all the layers in the neural network have been
traversed. The weights learned this way are fine-tuned and used as the initial
weights to train the neural network.

Pretraining the third hidden layer
[https://cdn.sanity.io/images/vr8gru94/production/1b072c8eb9552e53a120b6815775564d0153cd2d-1000x563.png]
Learning the initial weights for the third hidden layer (Image by the author)

We can parse the terms now that we understand how this algorithm works. This
approach is greedy because it does not optimize the initial weights across all
layers in the network but only focuses on the current layer. The weights are
learned layerwise in an unsupervised setting. The term pretraining signifies
that this process occurs ahead of the actual training process.

This approach to weight initialization was widely used in the deep learning
research community before the advent of newer weight initialization techniques
that do not require pretraining.


ZERO OR CONSTANT INITIALIZATION

The need for a complex algorithm like the greedy layerwise unsupervised
pretraining for weight initialization suggests that trivial initializations
don’t necessarily work.

This section will explain why initializing all the weights to a zero or constant
value is suboptimal. Let’s consider a neural network with two inputs and one
hidden layer with two neurons, and initialize the weights and biases to zero, as
shown.

Nueral network
[https://cdn.sanity.io/images/vr8gru94/production/dd8f304efc55a3954168ad13943dba0267c77166-1000x563.png]
A simple neural network with one hidden layer, with the biases set to zero
(Image by the author)

For this neural network, a11a_{11}a11 and a12a_{12}a12 are given by the
following equations:

a11=w11x1+w12x2a_{11} = w_{11}x_1 + w_{12}x_2a11 =w11 x1 +w12 x2
a12=w21x1+w22x2a_{12} = w_{21}x_1 + w_{22}x_2a12 =w21 x1 +w22 x2
Setting w11,w12,w21,and w22 to 0,Setting \text{ }w_{11}, w_{12}, w_{21}, and
\text{ }w_{22} \text{ }to\text{ } 0,Setting w11 ,w12 ,w21 ,and w22  to 0,
a11=a12=0a_{11} = a_{12} = 0a11 =a12 =0
  ⟹  h11=σ(a11)=0\implies h_{11} = \sigma{(a_{11})} = 0⟹h11 =σ(a11 )=0
and h12=σ(a12)=0and\text{ } h_{12} = \sigma{(a_{12})} = 0and h12 =σ(a12 )=0

Let σ\sigmaσ denote the activation function.

Given a loss function L\mathcal{L}L, the updates that each of the weights in the
neural network receives during backpropagation are computed as follows:

∇w11=∂L∂y.∂y∂h11.∂h11∂a11.x1\nabla w_{11} =
\frac{\partial{\mathcal{L}}}{\partial{y}}.\frac{\partial{y}}{\partial{h_{11}}}.\frac{\partial{h_{11}}}{\partial{a_{11}}}.x_1∇w11
=∂y∂L .∂h11 ∂y .∂a11 ∂h11 .x1
∇w21=∂L∂y.∂y∂h12.∂h12∂a12.x1\nabla w_{21} =
\frac{\partial{\mathcal{L}}}{\partial{y}}.\frac{\partial{y}}{\partial{h_{12}}}.\frac{\partial{h_{12}}}{\partial{a_{12}}}.x_1∇w21
=∂y∂L .∂h12 ∂y .∂a12 ∂h12 .x1
But h11=h12 (since a11=a12)But\text{ } h_{11} = h_{12} \text{ }(since \text{ }
a_{11} = a_{12})But h11 =h12  (since a11 =a12 )
  ⟹   ∇w11=∇w21\implies \text{ }\nabla w_{11} = \nabla w_{21}⟹ ∇w11 =∇w21

After the first update, the weights w11w_{11}w11 and w21w_{21}w21 move away from
zero but are equal.

Similarly, we see that the weights w12w_{12}w12 and w22w_{22}w22 are equal after
the first update.

∇w11=∂L∂y.∂y∂h11.∂h11∂a11.x1\nabla w_{11} =
\frac{\partial{\mathcal{L}}}{\partial{y}}.\frac{\partial{y}}{\partial{h_{11}}}.\frac{\partial{h_{11}}}{\partial{a_{11}}}.x_1∇w11
=∂y∂L .∂h11 ∂y .∂a11 ∂h11 .x1
∇w22=∂L∂y.∂y∂h12.∂h12∂a12.x2\nabla w_{22} =
\frac{\partial{\mathcal{L}}}{\partial{y}}.\frac{\partial{y}}{\partial{h_{12}}}.\frac{\partial{h_{12}}}{\partial{a_{12}}}.x_2∇w22
=∂y∂L .∂h12 ∂y .∂a12 ∂h12 .x2
But h11=h12 (since a11=a12)But\text{ } h_{11} = h_{12} \text{ }(since \text{
}a_{11} = a_{12})But h11 =h12  (since a11 =a12 )
  ⟹   ∇w12=∇w22\implies \text{ } \nabla w_{12} = \nabla w_{22}⟹ ∇w12 =∇w22

The weights are initially equal and receive the same update at each step. The
neurons, therefore, evolve symmetrically as the training proceeds, and we will
not be able to break this symmetry. This is true even when the weights are
initialized to any constant k. The weights are initially at k, then receive the
same update, leading to the symmetry problem yet again!

But why is this a problem?

The main advantage of using a neural network over traditional machine learning
algorithms is its ability to learn a complex mapping from the input space onto
the output. It is for this reason neural networks are called . The various
parameters of the network (weights) enable the neurons in the different layers
to learn other aspects of this mapping. However, so long as the weights flowing
into a neuron stay equal, all the neurons in a layer learn the “same” thing.
Such a model performs poorly in practice.

Key takeaway: Under zero or constant weight initialization, the neurons in a
layer change symmetrically throughout the training process.

--------------------------------------------------------------------------------

📑 Use of Regularization in Neural Networks: When training deep neural networks,
you can use regularization techniques such as dropout to avoid overfitting.

If you implement dropout, a specific fraction of neurons in each layer are
randomly switched off during training. As a result, those neurons may not get
updates as the training proceeds, and it is possible to break the symmetry.

However, the scope of this tutorial is to explain how the weights in a neural
network should be carefully initialized in the absence of other regularization
techniques.

--------------------------------------------------------------------------------


RANDOM INITIALIZATION

Given that we cannot initialize the weights to all zeros or any constant k, the
next natural step is to initialize them to random values. But does random
initialization work?


INITIALIZING THE WEIGHTS TO SMALL RANDOM VALUES

Let’s try initializing the weights to small random values. We’ll take an example
to understand what happens in this case.

import numpy as np



Consider a neural network with five hidden layers, each with 50 neurons. The
input to the network is a vector of length 100.

# x: input vector 
x = np.random.randn(1,100) 
# 5 hidden layers each with 50 neurons
hidden_layers = [50]*5
use_activation = ['tanh']*len(hidden_layers)
# available activations
activation_dict = {'tanh':lambda x:np.tanh(x),'sigmoid':lambda x:1/(1+np.exp(-x))}
H_mat = {}



Let’s observe what happens during the forward pass through this network. The
weights are drawn from a standard normal distribution with zero mean and unit
variance, and they’re all scaled by a factor of 0.01.

for i in range(len(hidden_layers)):
  if i == 0:
    X = x
  else:
    X = H_mat[i-1]

  # define fan_in and fan_out 
  fan_in = X.shape[1]
  fan_out = hidden_layers[i]

  # weights are small random values
  W = np.random.randn(fan_in,fan_out)*0.01

  H = np.dot(X,W)
  H = activation_dict[use_activation[i]](H)
  H_mat[i] = H



For small random values of weights, we observe that the activations grow smaller
as we go deeper into the neural network.

tanh activations across the hidden layers
[https://cdn.sanity.io/images/vr8gru94/production/b586a937289245717258208dab86f86127f08f4f-1000x173.png]
Vanishingly small activations in the deeper layers of the network

During backpropagation, the gradients that flow into a neuron are proportional
to the activation they receive. When the magnitude of activations is small, the
gradients are vanishingly small, and the neurons do not learn anything!


INITIALIZING THE WEIGHTS TO LARGE RANDOM VALUES

Let’s try initializing the weights to larger random values. Replace the weight
matrix with the following W, where the samples are drawn from a standard normal
distribution.

W = np.random.randn(fan_in,fan_out)



When the weights have a large magnitude, the sigmoid and tanh activation
functions take on values very close to saturation, as shown below. When the
activations become saturated, the gradients move close to zero during
backpropagation.

sigmoid and tanh activations
[https://cdn.sanity.io/images/vr8gru94/production/aa90266d28481ee878f9fe5b2bafbe3dc93c4a04-1000x451.png]
Saturation of sigmoid and tanh activations (Image by the author)
tanh activations for large random weights
[https://cdn.sanity.io/images/vr8gru94/production/3251d0c912e24b0c9e9cb0fb059fbedf48145f0d-1000x171.png]
Saturating tanh activations for large random weights

Let’s summarize the observations from the above experiments.

 1. In the first case, we sampled the initial weights from a standard normal
    distribution with zero mean and unit variance and scaled them by a factor of
    0.01. This is equivalent to drawing samples from a standard normal
    distribution with zero mean and variance (0.01)2(0.01)^2(0.01)2, which is
    negligibly small. When the weight distribution has a small variance, both
    activations during forward pass and gradients during backprop vanish.
 2. In the second case, we sampled from a standard normal distribution, without
    scaling the samples. We faced the problem of saturating activations and
    vanishing gradients during backpropagation.

However, suppose we pick the optimal scaling factor, or equivalently, find the
optimal variance of the weight distribution. In that case, we can get the
network to operate in the region between vanishing and saturating activations.


A BETTER WEIGHT INITIALIZATION STRATEGY

Let us assume that the inputs have been normalized to have zero mean and unit
variance. The weights are drawn from a distribution with zero mean and a fixed
variance. But what should that variance be? Let’s analyze!

To compute the optimal variance, we’ll use a11a_{11}a11 as the first input to
the first neuron in the second hidden layer, instead of h11=σ(a11)h_{11} =
\sigma(a_{11})h11 =σ(a11 ). h11h_{11}h11 is proportional to a11a_{11}a11 , so we
ignore the explicit effect of activations to simplify the derivation.

Weights flowing into the first neuron
[https://cdn.sanity.io/images/vr8gru94/production/f732796b4f3eba84dc55ee35c578cf16038f1902-1000x563.png]
Weights flowing into the first neuron in the first hidden layer (Image by the
author)
a11=∑i=1nw1ixia_{11} = \sum_{i=1}^{n} w_{1i}x_ia11 =i=1∑n w1i xi
Var(a11)=Var(∑i=1nw1ixi)Var(a_{11}) = Var\left(\sum_{i=1}^{n}
w_{1i}x_i\right)Var(a11 )=Var(i=1∑n w1i xi )

Assumption: The weights and inputs are .

Var(a11)=∑i=1nVar(w1ixi)Var(a_{11}) = \sum_{i=1}^{n} Var(w_{1i}x_i)Var(a11
)=i=1∑n Var(w1i xi )
  ⟹  Var(a11)=∑i=1n{(E[w1i])2Var(xi)+(E[xi])2Var(w1i)+Var(xi)Var(w1i)\implies
Var(a_{11}) = \sum_{i=1}^{n} \{ (\mathbb{E}[w_{1i}])^2 Var(x_i) +
(\mathbb{E}[x_i])^2 Var(w_{1i}) + Var(x_i)Var(w_{1i})⟹Var(a11 )=i=1∑n {(E[w1i
])2Var(xi )+(E[xi ])2Var(w1i )+Var(xi )Var(w1i )

Substituting E[w1i]=0\mathbb{E}[w_{1i}] = 0E[w1i ]=0 and E[xi]=0\mathbb{E}[x_i]
= 0E[xi ]=0 in the above equation:

Var(a11)=∑i=1nVar(xi)Var(w1i)Var(a_{11}) = \sum_{i=1}^{n}
Var(x_i)Var(w_{1i})Var(a11 )=i=1∑n Var(xi )Var(w1i )
Let Var(xi)=Var(x) and Var(w1i)=Var(w1)Let \text{ } Var(x_i) = Var(x) \text{ }
and \text{ } Var(w_{1i}) = Var(w_1)Let Var(xi )=Var(x) and Var(w1i )=Var(w1 )
  ⟹  Var(a11)=n.Var(w)Var(x)\implies Var(a_{11}) = n.Var(w)Var(x)⟹Var(a11
)=n.Var(w)Var(x)

Let’s compute the variance of a21a_{21}a21 in the second hidden layer.

Variance in the second hidden layer
[https://cdn.sanity.io/images/vr8gru94/production/33ad956f0d9cfe8f2190694960021a3cd49f9e1a-1000x563.png]

Var(a21)=∑i=1nVar(a1i)Var(w2i) (1)Var(a_{21}) = \sum_{i=1}^{n}
Var(a_{1i})Var(w_{2i}) \text{ }(1)Var(a21 )=i=1∑n Var(a1i )Var(w2i ) (1)
Substituting Var(a11)=n.Var(w)Var(x) andSubstituting\text{ } Var(a_{11}) =
n.Var(w)Var(x) \text{ }andSubstituting Var(a11 )=n.Var(w)Var(x) and
Var(w1i)=Var(w2)Var(w_{1i}) = Var(w_2)Var(w1i )=Var(w2 )
  ⟹  Var(a21)=n.Var(w2)n.Var(w1)Var(x)\implies Var(a_{21}) =
n.Var(w_2)n.Var(w_1)Var(x)⟹Var(a21 )=n.Var(w2 )n.Var(w1 )Var(x)
Var(a21)=n2[Var(w)]2Var(x)Var(a_{21}) = n^2[Var(w)]^2Var(x)Var(a21
)=n2[Var(w)]2Var(x)

By induction, the variance of input to neuron i in the hidden layer k is given
by:

Var(aki)=[n.Var(w)]kVar(x)Var(a_{ki}) = [n.Var(w)]^kVar(x)Var(aki
)=[n.Var(w)]kVar(x)

To ensure that the quantity n.Var(w) neither vanishes nor grows exponentially
(leading to instability in the training process), we need n.Var(w) =1.

n.Var(w)=1n. Var(w) = 1n.Var(w)=1
Var(w)=1nVar(w) = \frac{1}{n}Var(w)=n1
If Var(X)=σ2, then Var(c.X)=c2σ2If \text{ }Var(X) = \sigma^2, \text{ }then
\text{ }Var(c.X) = c^2\sigma^2If Var(X)=σ2, then Var(c.X)=c2σ2

To achieve this, we can sample the weights from a standard normal distribution
and scale them by a factor of 1n\frac{1}{\sqrt{n}}n 1 .


XAVIER OR GLOROT INITIALIZATION

X. Glorot and Y. Bengio proposed an improved weight initialization strategy
named the Xavier or Glorot initialization [2] (after the researcher Xavier
Glorot).

In a neural network, the number of weights that flow into each neuron in a
neural network is called faninfan_{in}fanin , and the number of weights that
flow out of the neuron is called fanoutfan_{out}fanout .

Explaining fan_in and fan_out
[https://cdn.sanity.io/images/vr8gru94/production/3a4fd7772248db6b2540325aba62d3d2177d4d7d-1000x563.png]
Explaining fan_in and fan_out (Image by the author)

When the weight distribution’s variance is set to 1fanin\frac{1}{fan_{in}}fanin
1 , the activations neither vanish nor saturate during the forward pass.

fanin.Var(w)=1fan_{in}. Var(w) = 1fanin .Var(w)=1
Var(w)=1faninVar(w) = \frac{1}{fan_{in}}Var(w)=fanin 1

However, during backpropagation, the gradients flow backward through the full
network from the output layer to the input layer. We know that the
fanoutfan_{out}fanout of a neuron is the number of weights that flow out of the
neuron into the next layer. But the fanoutfan_{out}fanout of a particular neuron
is also the number of paths through which gradients flow *into* it during
backpropagation. Therefore, having the variance of the weights equal to
1fanout\frac{1}{fan_{out}}fanout 1 helps overcome the problem of vanishing
gradients.

To account for both the forward pass and backprop, we do the following: When
computing the variance, instead of faninfan_{in}fanin or fanoutfan_{out}fanout ,
we consider the average of faninfan_{in}fanin and fanoutfan_{out}fanout .

fanin+fanout2.Var(w)=1\frac{fan_{in}+fan_{out}}{2}. Var(w) = 12fanin +fanout
.Var(w)=1
Var(w)=2fanin+fanoutVar(w) = \frac{2}{fan_{in}+fan_{out}}Var(w)=fanin +fanout 2

A random variable that is uniformly distributed in an interval centered around
zero has zero mean. So we can sample the weights from a uniform distribution
with variance 2fanin+fanout\frac{2}{fan_{in}+fan_{out}}fanin +fanout 2 . But how
do we find the endpoints of the interval?

A continuous random variable A that is uniformly distributed in the interval
[-a,a] has zero mean and variance of a23\frac{a^2}{3}3a2 .

Equation
[https://cdn.sanity.io/images/vr8gru94/production/7af991fbf6ea4d0f95fc02e55153e33cd67c5b47-1000x563.png]

Var(A)=(2a)212=a23Var(A) = \frac{(2a)^2}{12} = \frac{a^2}{3}Var(A)=12(2a)2 =3a2

We know that the variance should be equal to
2fanin+fanout\frac{2}{fan_{in}+fan_{out}}fanin +fanout 2 ; we can work backward
to find the endpoints of the interval.

Var(w)=(2a)212=a23Var(w) = \frac{(2a)^2}{12} = \frac{a^2}{3}Var(w)=12(2a)2 =3a2
We have, We\text{ }have,\text{ }We have, 
Var(w)=2fanin+fanoutVar(w) = \frac{2}{fan_{in}+fan_{out}}Var(w)=fanin +fanout 2
  ⟹  a23=2fanin+fanout\implies \frac{a^2}{3} = \frac{2}{fan_{in}+fan_{out}}⟹3a2
=fanin +fanout 2
a2=6fanin+fanouta^2 = \frac{6}{fan_{in}+fan_{out}}a2=fanin +fanout 6
  ⟹  a=6fanin+fanout\implies a = \sqrt{\frac{6}{fan_{in}+fan_{out}}}⟹a=fanin
+fanout 6
w∈U[−6fanin+fanout,6fanin+fanout]w \in
\mathcal{U}\left[-\sqrt{\frac{6}{fan_{in}+fan_{out}}},\sqrt{\frac{6}{fan_{in}+fan_{out}}}\right]w∈U[−fanin
+fanout 6 ,fanin +fanout 6 ]


📑 GLOROT INITIALIZATION IN KERAS

To implement Glorot initialization in your deep learning models, you can use
either the GlorotUniform or GlorotNormal class in the Keras initializers module.
If you do not specify a kernel initializer when defining the model, it defaults
to GlorotUniform.

 * The GlorotNormal class initializes the weight tensors with samples from a
   truncated normal distribution with variance
   2fanin+fanout\frac{2}{fan_{in}+fan_{out}}fanin +fanout 2 . When samples are
   drawn from a “truncated” normal distribution, samples that lie farther than
   two standard deviations away from the mean are discarded.
 * The GlorotUniform class initializes the weight tensors by sampling from a
   uniform distribution in the interval
   [−6fanin+fanout,6fanin+fanout]\left[-\sqrt{\frac{6}{fan_{in}+fan_{out}}},\sqrt{\frac{6}{fan_{in}+fan_{out}}}\right][−fanin
   +fanout 6 ,fanin +fanout 6 ].

--------------------------------------------------------------------------------


HE INITIALIZATION

It was found that Glorot initialization did not work for networks that used ReLU
activations as the backflow of gradients was impacted [3].

But why does this happen?

Unlike the sigmoid and tanh activations, the ReLU function, which maps all
negative inputs to zero: ReLU(x) = max(0,x), does not have a zero mean.

ReLU Activation
[https://cdn.sanity.io/images/vr8gru94/production/a547acaadb482f996d00a7ecb9c4169c38c8d3e5-1000x563.png]


The ReLU function, therefore, outputs 0 for one-half of the input spectrum,
whereas tanh and sigmoid activations give non-zero outputs for all values in the
input space. Kaiming He et al. introduced a new initialization technique that
takes this into account by introducing a factor of 2 when computing the variance
[4].

fanin2.Var(w)=1\frac{fan_{in}}{2}. Var(w) = 12fanin .Var(w)=1
Var(w)=2faninVar(w) = \frac{2}{fan_{in}}Var(w)=fanin 2

As with Glorot initialization, we can also draw the weights from a uniform
distribution for He initialization.

Var(w)=(2a)212=a23Var(w) = \frac{(2a)^2}{12} = \frac{a^2}{3}Var(w)=12(2a)2 =3a2
We have, We\text{ }have,\text{ }We have, 
Var(w)=2faninVar(w) = \frac{2}{fan_{in}}Var(w)=fanin 2
  ⟹  a23=2fanin\implies \frac{a^2}{3} = \frac{2}{fan_{in}}⟹3a2 =fanin 2
a2=6fanina^2 = \frac{6}{fan_{in}}a2=fanin 6
  ⟹  a=6fanin\implies a = \sqrt{\frac{6}{fan_{in}}}⟹a=fanin 6
w∈U[−6fanin,6fanin]w \in
\mathcal{U}\left[-\sqrt{\frac{6}{fan_{in}}},\sqrt{\frac{6}{fan_{in}}}\right]w∈U[−fanin
6 ,fanin 6 ]

--------------------------------------------------------------------------------


📑 HE INITIALIZATION IN KERAS

The Keras initializers module provides the HeNormal and HeUniform for He
initialization.

 * The HeNormal class initializes the weight tensors with samples drawn from a
   truncated normal distribution with zero mean and variance
   2fanin\frac{2}{fan_{in}}fanin 2 .
 * The HeUniform class initializes the weight tensors with samples drawn from a
   uniform distribution in the interval
   [−6fanin,6fanin]\left[-\sqrt{\frac{6}{fan_{in}}},\sqrt{\frac{6}{fan_{in}}}\right][−fanin
   6 ,fanin 6 ].

--------------------------------------------------------------------------------

Considering the fanoutfan_{out}fanout when initializing weights, you can draw
the weights from normal distribution with the following variance:

fanin+fanout22.Var(w)=1\frac{\frac{fan_{in}+fan_{out}}{2}}{2}.Var(w) = 122fanin
+fanout .Var(w)=1
Var(w)=4fanin+fanoutVar(w) = \frac{4}{fan_{in}+fan_{out}}Var(w)=fanin +fanout 4

Equivalently, you may sample the initial weights from a uniform distribution
with variance 4fanin+fanout\frac{4}{fan_{in}+fan_{out}}fanin +fanout 4 .

Var(w)=(2a)212=a23Var(w) = \frac{(2a)^2}{12}= \frac{a^2}{3}Var(w)=12(2a)2 =3a2
We have,We\ have,We have,
Var(w)=4fanin+fanoutVar(w) = \frac{4}{fan_{in}+fan_{out}}Var(w)=fanin +fanout 4
  ⟹  a23=4fanin+fanout\implies \frac{a^2}{3} = \frac{4}{fan_{in}+fan_{out}}⟹3a2
=fanin +fanout 4
a2=12fanin+fanouta^2 = \frac{12}{fan_{in}+fan_{out}}a2=fanin +fanout 12
  ⟹  a=12fanin+fanout\implies a = \sqrt{\frac{12}{fan_{in}+fan_{out}}}⟹a=fanin
+fanout 12
w∈U[−12fanin+fanout,12fanin+fanout]w \in
\mathcal{U}\left[-\sqrt{\frac{12}{fan_{in}+fan_{out}}},\sqrt{\frac{12}{fan_{in}+fan_{out}}}\right]w∈U[−fanin
+fanout 12 ,fanin +fanout 12 ]


SUMMING UP

I hope this tutorial helped you understand the importance of weight
initialization when training deep learning models.

 * Initializing the weights to zero or a constant value leads to the
   symmetry-breaking problem. This problem stems from the weights receiving the
   same updates at each step and updating symmetrically as the training
   proceeds.
 * Initializing the weights to small random values leads to the problem of
   vanishing gradients. This is because the gradients flowing into a particular
   neuron are proportional to the activation that it receives. On the other
   hand, initializing the weights to large random values causes the activations
   to get saturated, resulting in vanishing gradients during backpropagation.
 * To prevent the weights from being drawn from a distribution whose variance is
   neither too large nor too small, the variance of the distribution must be
   approximately 1.
 * Xavier or Glorot initialization works well for networks using activations
   with zero mean, such as the sigmoid and tanh functions.
 * When using ReLU activation that does not have zero mean, it’s recommended to
   use the He initialization.

When using deep learning in practice, you can experiment with weight
initialization, regularization, and techniques such as to improve the neural
network’s training process. Happy learning and coding!


RESOURCES

[1] G. E. Hinton, R. R. Salakhutdinov, (2006), Sciences

[2] X. Glorot, Y. Bengio, (2010), AISTATS 2010

[3] K. Kumar, (2017)

[4] He et al., (2015), CVPR 2015

[5] , Gray and Davisson

[6] , Boyd and Vandenberghe

[7] , Advanced Machine Learning Systems, Cornell University, Fall 2021

[8] , keras.io

Share via:


FURTHER READING


Author
[/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&w=3840&q=100]

Bala Priya C

Technical Writer

--------------------------------------------------------------------------------

Jump to section
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 

Share via:


Product

Solutions

Resources

Company

Legal


© Pinecone Systems, Inc. | San Francisco, CA

Pinecone is a registered trademark of Pinecone Systems, Inc.

[/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fpopup-bg.197eeb2c.png&w=3840&q=100]


INTRODUCING — PINECONE SERVERLESS

Build knowledgeable AI at up to 50x lower cost. No need to manage
infrastructure. Get started with $100 in usage credits.

